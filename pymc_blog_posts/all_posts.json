[
  {
    "title": "Stochastic Volatility Model with PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/01-xpost-tw-stochastic-volatility",
    "date": null,
    "author": null,
    "description": "Explore the concept of time-varying volatility in asset prices, modeled using a stochastic process. The example demonstrates the computation of this volatility based on the daily returns of the S&P 500 using PyMC.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "MCMC sampling for dummies",
    "url": "https://www.pymc-labs.com/blog-posts/02-xpost-tw-MCMC-sampling",
    "date": null,
    "author": null,
    "description": "Explore the intuition behind MCMC sampling and the random-walk Metropolis algorithm through code examples rather than complex formulas or math-speak.",
    "content": "MCMC sampling for dummies\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nHow is inference actually performed and how does it work? How do we get these magical samples from the posterior?\nThe beauty of probabilistic programming is that you actually don't have to understand how the inference works in order to build models, but it certainly helps.\nMath and statisics tend to seem complex because when they are taught,\nno one ever tells you about the intuition behind the concepts (which is usually quite simple) but only hands you some scary math.\nThis blog post is an attempt at trying to explain the intuition behind MCMC sampling (specifically, the random-walk Metropolis algorithm).\nCritically, we'll be using code examples rather than formulas or math-speak.\n\nFor the full example, see:MCMC sampling for dummies",
    "word_count": 129
  },
  {
    "title": "NBA Foul Analysis with Item Response Theory using PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/03-xpost-ar-nba-irt",
    "date": null,
    "author": null,
    "description": "Delve into the use of Bayesian Item Response Theory and the Rasch model for analyzing NBA foul calls data. The model estimates individual player contributions to foul outcomes.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Building Time-Series Models With Known Data Structure",
    "url": "https://www.pymc-labs.com/blog-posts/04-xpost-be-time-series-volcano",
    "date": null,
    "author": null,
    "description": "Dive into the application of Gaussian Processes in constructing time-series models, leveraging domain knowledge of the underlying time-series. The discussion includes the use of PyMC for modeling carbon dioxide measurements derived from ice cores and atmospheric readings from Mauna Loa, a Hawaiian volcano.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "PyMC, Aesara and AePPL: The New Kids on The Block",
    "url": "https://www.pymc-labs.com/blog-posts/2022-07-10-ricardo-video",
    "date": null,
    "author": null,
    "description": "Dive into the world of PyMC, Aesara, and AePPL, the new powerhouses in probabilistic programming. Discover how they revolutionize Bayesian modeling and open up new possibilities for data analysis.",
    "content": "PyMC, Aesara and AePPL: The New Kids on The Block\n\nSeptember 23, 2025\n\nByRicardo Vieira\n\nIntroduction\n\nIn July, 2022, PyMC Labs hosted a talk about PyMC, Aesara, and AePPL: the new kids on the block. Ricardo Vieira explored the inner workings of the newly released version of PyMC (v 4.0). He looked at the Aesara backend, focusing on the brand newRandomVariableoperators, which are the foundation of PyMC models. He then talked about a self-contained PPL project (Aeppl) dedicated to converting Aesara RandomVariable graphs to probability functions and then, how PyMC makes use of these two packages to create efficient random generator functions and probability evaluation functions, with the ultimate goal of facilitating a fully-fledged modern Bayesian workflow.\n\nPyMCis a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods.\n\nAesarais a Python library that allows users to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Aesara is based on Theano (https://github.com/Theano/Theano), which has been powering large-scale computationally intensive scientific investigations since 2007.\n\nAePPLis a new library focused on converting (arbitrary)  graphs containing Aesara RandomVariables into joint log-probability graphs. It can understand complex graphs that include nested operations, conditionals, loops, and advanced indexing, allowing one to generate rich probability functions automatically without having to muddle through the mathematical details.\n\nAbout Speaker\n\nRicardo Vieira is a PyMC developer and data scientist at PyMC Labs. He spent several years teaching himself Statistics and Computer Science at the expense of his official degrees in Psychology and Neuroscience.\n\nGitHub:https://github.com/ricardoV94/\n\nTwitter:https://twitter.com/RicardoV944\n\nWebsite:https://ricardov94.github.io/posts/\n\nResources\n\nPyMC V4 Release announcement\n\nPyMC V4 Release notes\n\nPyMC website\n\nAbout Aesara\n\nAbout Aeppl\n\nOutline\n\nIntroduction\n\nAesara and random variables\n\nAePPL and probabilities\n\nPyMC and the modern Bayesian workflow\n\nQ&A\n\nThe timestamps below provide a detailed outline of the talk.\n\nTimestamps\n\nPart 1: Introduction\n\n00:00Thomas Wiecki does introduction and background\n\n08:30Ricardo begins presentation\n\n08:381.0 Intro: PyMC, Aesara and AePPL: The New Kids on The Block\n\nPart 2:Aesara\n\n10:072.0 About Aesara\n\n12:042.1 Crash course on: Numpy-like tensor operations\n\n15:202.2 Compilation to different backends: Numba, JAX\n\n15:522.3 Automatic differentiation\n\n16:552.4 Automatic computational stabilization and specialization\n\n18:352.5 User friendly graph manipulation\n\n20:122.6 Random variables (scalar, constant, shared variables in Aesara, create a normal random variable using Aesara, random number generator)\n\n24:05PyMC has a utility to create an Aesara function that updates seeds automatically\n\n24:40Q: no questions so far\n\n25:283.0 PyMC (random side); How PyMC uses Aesara functionality to do two of the most important operations in the Bayesian workflow, which are: doing prior predictive draws from a model & taking posterior predictive draws from the same model\n\n25:483.1 Distributions in PyMC are just functions which return random variables\n\n26:52Handy way to debug models; helper function: pm.draw(x, draws=2)\n\n27:353.2 A PyMC model is just an Aesara graph composed of multiple RandomVariables\n\n29:053.3 Do prior predictive modeling with PyMC models\n\n30:323.4 Do posterior predictive modeling with PyMC models\n\n32:03Q: no questions\n\nPart 3: AePPL\n\n32:354.0 AePPL: a new library in ecosystem of PyMC and Aesara, \u201cAe\u201d is the prefix used; AePPL = Aesara Probabilistic Programming Language; Goal: convert Aesara graphs made of random variables into log probability graphs\n\n34:004.1 Crash course in AePPL: Convert RandomVariable graphs into log-probability graphs\n\n37:004.2 Create log-probability graphs conditioned on (arbitrary) operations\n\nPart 4: Q&A\n\n44:30Q: Is there a PyMC example using a Gaussian random model with a unique distribution argument?\n\n44:55Q: Can PyMC now automatically exploit conjugacy during sampling? (AeMCMC)\n\n45:385.0 PyMC: probability side\n\n46:145.1 PyMC uses AePPLl to transform RandomVariable graphs into log-probability graphs\n\n49:245.2 Sampling (or optimizing). Once you have the densities and gradients, you can start doing Bayesian inference. pm.find_MAP()\n\n50:455.3 PyMC uses AePPL to build new distribution types (censoring process) (pm.Censored)\n\n54:105.4 Users will be able to define arbitrary distributions (WIP)\n\n58:50Q: Which composition estimator does AePPL not yet support?\n\n01:00:05Q: relationship between AePPL and Aesara\n\n01:02:10Q: How difficult would it be to support other data types?\n\nTranscript\n\nThomas Wiecki: 00:00:00 Introduction and overview of PyMC\n\nAll right welcome everybody to our PyMC webinar. I\u2019m very excited to have you all\nhere and to present our amazing speaker Ricardo, who will be talking about PyMC and AePPL, pronounced AePPLe and the new kids on the block.  I wanted to call it the cool kids on the block but I was dismissed.\nBefore I introduce Ricardo and we get into the actual talk I just wanted to basically give a bit of background and some announcements. So Ricardo and I we both work at PyMC labs and this is our amazing cool new bayesian consultancy, which basically is a whole bunch of the PyMC core developers teaming up to solve whatever hardcore business problems we can come across on the bayesian modeling and PyMC 2 and I really couldn't be more happy about the team that we have. I mean, it's all of us who have been working together for many years already on PyMC and now basically redirecting that to solve your business problems and yeah, so very happy with the people we have from all kinds of different backgrounds most of us actually coming originally from academia and yeah that's where we fell in love with based in modeling now can turn that into our day job.\nWe mostly while we do a whole bunch of stuff all centered around PyMC and bayesian modeling just a couple of to highlight, I mean one thing actually that fairly often is something that we get requests for is speeding up models like someone already has built a PyMC model and they're really happy with it but it takes six hours to run and then we I don't know get that down to 30 minutes, 10 minutes and then from there often that allows more breathing room to actually then extend the model further and add more structure to it make it hierarchical and all in the purpose of maximizing the value of that the model has right like improving the accuracy but yeah and the other thing of course that we do is training and workshops so these have been very popular this is just yeah if you are interested in that just email methomaswiecki@pymc-labs.org, you can also follow me on twitter. Here are just some links to our website and there's a whole bunch of blog posts pymclabs.io and that's where we also find more information about the workshops. Here are a whole bunch of links for general PyMC, so if you have questions on the software then you can find us on discourse of course the code and everything lives on the GitHub our main page actually is listed here but it's pymc.io so that's really the main entry point and then it will have links to everything else.\nAnnouncements you can find on our twitter for PyMC it's pymc_devs, we also have a labs account, pymc labs. We are on linkedin if you hang out there and then our youtube channel where this talk will be posted and a whole bunch of other materials and top box can be found and we have a code of conduct which we asked everyone to follow which you can read up on here.\n\n00:03:38 Bayesian Course by Thomas, Ravin and Alex\n\nAlso, we have a bayesian course,Intuitive Bayes Introductory Course, that we developed, this is with Ravin Kumar and Alex andorra and this is basically a self-paced learning course where you really focus on teaching the intuitions behind it and using that illustratively through code and using PyMC labs rather than just like give you the math like this is really the result of my own frustrations with how I had to learn basic modeling with like text books and like conjugate priors and all kinds of obscure stuff that actually no one really cares about toda,  so this is like the the updated version that really like cuts to the chase and hopefully gets you a lot to to start quickly quickly and with the focus for data scientists and python developers so people who already know python but maybe have frequent this background so this is who it's geared towards and it's using videos in\ncode so that's intuitivebase.com.\n\n00:04:42 PyMC 4.0 and its features\n\nWell, this is really what we'll be talking about a lot today. PyMC 4.0 actually now already 4.1. We couldn't be more excited about this release. It's been taking us a really long time to get there,\nit's more or less like a major rewrite of PyMC and of the core underlying features and this is really what Ricardo's talk is going to be about. Some of the really exciting features\nare the new jacks back-end so they are also with PyMC labs we've been able to\nnow like build models of like unparalleled scale where with hundreds of thousands of data points hundreds of thousands of parameters like very complex hierarchical custom process models but on the gp, on the gpu with jack sampling they fit in an hour so the\nscale is just something completely new, so gpu support is something that is like really exciting.\nBut also as you might have known Theano has been discontinued so we had to find a new back end so we decided to fork it and this is work that Bretton Willard is leading is now called Aesara which we'll be hearing much more about and a whole bunch of like things that have always been annoying for mainly the developers and we're now fixed where we really have like such a much more powerful base to build on and really enhance many things and there's like better nuts initialization whole and also actually our installation process is much more simplified thanks to Ben Morris who has just joined so you can really just do conda install -c  conda-forge pymc and no matter which platform it should just give you the compilers and everything and you shouldn't need to worry about anything so yeah that is cool.\n\n00:06:42 Thomas introduces Ricardo\n\nAnd so now let me introduce Ricardo who's a good friend of mine and co-developer of PyMC and a colleague at PyMC Labs, so he really has been like the main developer of PyMC in at least the last year and PyMC 4.0 is largely to his hard work on this in like all the different domains and also then Aesar and AePPL so there really couldn't be a better person to talk about really the technical underpinnings , so yeah it's really cool to have him here. He is self-taught which is really impressive for what he's able to do in statistics and computer science at the expense of his official degrees in psychology and neuroscience. So like many of us, he's a recovering academic. Follow him on GitHub, it's actually hard to keep up with him there so you can also follow with twitter for announcements. He has a cool blog and of course the PyMC labs website\nso with that I give it over to Ricardo to tell you about all the cool new developments and really the things underneath the hood and what makes PyMC 4.0 as powerful as it is, so with that thank you.\nIt\u2019s actually big enough but if you just zoom in once or twice it'll probably be even easier to read,yeah I was thinking about that,  I'll put it right here. Yeah, okay that's perfect.\n\n00:08:38 PyMC, Aesara, AePPL: The new kids on the block\n\nAll right, thanks for the very nice intro. If you folks want to check the notebook, I think it's just slightly updated from the presentation you can just write this link, I'll share it in the chat. So today I will give an overview of how PyMC 4.0 is built and mainly how it makes use of two very\ncool libraries Aesara and AePPL or AePPL, we are still debating the name and to give the cool functionality that PyMC offers for the whole bayesian workflow. I will assume you already used PyMC or you are familiar with the PyMC api so I will not dwell on that, I will just use it and actually during this talk I will perhaps show you how you would implement 90 percent of PyMC features by just understanding what Aesara and AePPL can offer you.\nAll right, this will be how we run this notebook live so let's hope it's fast enough. Here are the new or the cool kids on the block. We'll import AePPL, Aesara and PyMC with the common  aliases and I'll be using this version of PyMC 4.1.1.\nAesara 2.7.4 and AePPL 0.0.31 and also do import Aesara.tensor as at which is quite commonly used and I will just explain what we can do with these things. So Aesara we already had a bit of an introduction it's our fork of the Theano library which was a quite popular python library for tensor operations mostly used for deep learning back in the days but with which was actually much more flexible and hackable and the PyMC3 was built on top of Theano and now PyMC 4 is built on our fork of Theano, Aesara,  which still shares my most of the cool features that Theano had and also some new things.\n\n00:10:07 About Aesara\n\nSo just a quick overview, Aesara what it does is offers you a way to create graphs that\ndo tensor operate operations that follow the numpy api so things like broadcasting, indexing just like you are used with numpy and most of the functions you find in numpy you'll find in Aesara, it goes beyond numpy in that it tries to, it can be used to compile these graphs into very efficient code that you can call from PyMC we can compile to code and now to number and tracks as well and obviously to support software differentiation and also it has some routines to optimize graphs and to manipulate graphs which come very in handy in a probabilistic\nprogramming language like PyMC and we'll talk a bit about today, it also has a\nvery good support for graphs based on random variables to be able to take random draws from some distributions and then operate on them just like any other tensor variables.\n\n00:12:04 Numpy-like tensor operations\n\nSo very quick crash course on Aesara, I will go line by line and feel free to after I finish this section, just write questions you have in between and maybe I will answer one or two\nthat are more relevant. So, we'll just start by creating two input variables will be x which will be\na scalar variable it will be a float scalar variable and y will be a vector variable.\nWe'll combine them by just adding x plus y saving it in z and once we actually use these and evaluate a graph it will broadcast just like numpy so our scalar x will just broadcast into the shape of y which is a vector. i'm just giving names just to show then in the debug this is optional\nand then once we have z, I will just take the log usually you can do at dot some function with which is you have almost all the equivalent ones that you would have for numpy so if you do an numpy dot log you'll have a at.log and so we save the log of z in w and we'll just have a look at\nwhat this graph looks like by using Aesara dprint which stands for debug print. This is very helpful to understand Aesara graphs. It's a bit verbose and a bit confusing at first but you'll get used to it.\nSo we just start reading from the output w to the inputs which are nested inside\nso w is a log operation that's AePPLied element twice to all the values of w,\nthe input of this log is an addition operation, that's also element wise which has two inputs\ny our vector and x a dim shuffle of x which just means broadcasting x to the shape of y.\nThen once you have a graph and you actually want to do computations with it you call Aesara.function, you specify the inputs in our case x y and the output w\nand once in this process is where the graph is compiled by default. It will compile it to the c backend after you compile into a function you can then evaluate it in this case we'll just call with x equals zero and y one and e which you just take the log of y and we get output zero one.If you are in a hurry or you're just trying to debug or prototype you can call on any variable called that variable.eval and then pass the inputs in a dictionary and behind the scenes it will just compile the function and call it with these values and this is just handy if you just want to debug and see if it's giving you the numbers you would expect, and finally I presented you the graph going from x y to w but actually the tiles are you candefine any intermediate graph with any  number of inputs or outputs so we can also just say okay we'll use z as the input and w as the outputs instead of x and y as the inputs and now this is just a function which basically takes a vector because z is a vector and takes the logarithm of that vector and indeed we can compile evaluate and we get the logarithm of the what we passed as input.\n\n00:15:20 Compilation to different backends\n\nThe second cool thing is we can specify different computational backends for our\nfunctions by just passing a different mode. In this case we'll just compile this\nfunction to number and this is running number code when we call f number\nor numba, I never know how that should be pronounced and equivalent to jax\nwe can call it we'll get a warning that you can ignore for now but indeed this is all being computed in a Jax version and giving you back the output device array.\n\n00:15:55 Automatic differentiation\n\nThen the next big offering of files is automatic differentiation. Let's just do a simple graph which is a scalar x and take a logarithm of that and if you want a gradient you can just call at.grad specify a scalar cost and with respect to which variables you want the gradients so in this case we'll just get the gradient of y with respect to x, then we compile this function and you can actually have a function with multiple outputs. So we'll say we want y and the gradient of y with respect to x as the outputs and the input x so we get both the value and the gradient you can also dprint a function once it's compiled which will follow the same kind of d print you saw for graphs and we see we have two outputs just the log of x and the gradient which is a reciprocal of x if you remember it from calculus classes and indeed when you evaluate we got what we would expect.\n\n00:16:55 Automatic computational stabilization and specialization\nWhen you compile an Aesara functioning actually Aesara looks to see if your graph can be\noptimized or if you can be converted into an equivalent graph that's more numerically stable and this is really nice because you can just write the graph that you know is numerically correct, without having to worry about will this be like stable under float operations or is there another operator that would be faster to do this because as I will just replace these things when you compile it.  Here I'll show you an example, we're just taking a matrix x as an input and we'll take the logarithm of one minus the exponent of x this is an expression that appears so quite often when working with log probabilities or log cumulative functions and this is how the graph looks like when we just define it, then we can compile it and we can actually d print and we'll see the compiled graph looks quite different than the original one because it introduced this x specialized operator which is log one minus x which is just more numerical numerically stable than the naive way of computing a y like this.\nAnother way that Aesara  produces efficient code is it avoids recomputing the same operations twice. So if you have a function of two inputs and is the same function and the same inputs it will just compute it once. This only happens after compilation so here the graph has the two exponents of x and then adds them but once we compile it, it will be clever and we'll just compute the exponent of x once and then add it twice and yeah.\n\n00:18:40 User friendly graph manipulation\n\nNext step is Aesara, the reason we always talk about graphs and then functions that can be evaluated is that Aesara very strong graph manipulation features which are\nreally cool when you want to do graph manipulation which you often want to do in a bayesian setting especially if you are developing a library for users. So I'll just give a quick example again,\nsomething simple. Let's just have two scalar input variables x and lambda and I'll just create a variable that I call pdf of x which I think is just the pdf of an exponential distribution evaluated at lambda and x. I'll use the helpful evolve to just debug it I get the number and then if I wanted to say that instead of providing x I want to provide the logarithm of x this is for instance used if you are doing optimization and you want to be able to offer any value as input not just valid values that must be positive in the case of the exponential distribution, you can do that and so here what we do is we call is our clone replace we say I want as the output my pdf x graph this expression but now I want you wherever there was x I want you to actually use the exponent of x.Okay, so now when I evaluate this this expression I\u2019ll pass 0 instead of 1 but I'll get the same output because it will be just exponentiated before it's evaluated downstream.\n\n00:20:12 Random variables (scalar, constant, shared variables in Aesara, create a normal random variable using Aesara, random number generator)\n\nAnd last but not least, Aesara developed random variable operators which are all about creating graphs with that take random draws and they have a very nice well structured api that makes\nour life very easy in PyMC as we\u2019ll see later. This is going to look a bit messy but\njust bear with me I will just show you the minimal way of creating a random variable graph in either we'll start by creating a scalar location variable and a scale which will be a constant\nvariable instead of something we input it'll always be 1.0.\nI would be able to just use a python float or an integer or a numpy array but under the hood, it's good to know it's always converted to a constant in these cases so we just go ahead and create it directly. And then I will create another constant which will be a random number generator state which we just create with numpy random default rng, we pass a seed but instead of being a let's say a classical constant it will be a mutable constant which in Aesara are called shared variables, so you can think of this this would be a constant but we can change the value across calls and you'll see why this is useful in just a bit,  but so once we have these three variables we can go ahead and create a random normal variable which takes the location and scale as the first inputs, we specify size to be the default none just like numpy defaults to none, we give\nit a name that's optional and we pass the random number generator okay. The graph looks like this, it's just the inputs we had of the normal random variable in a different order. First comes the rng then this is the size none, it gets converted to an empty list. This is just a code for the type of the variable 11 stands for float64, this usually you can ignore and then the parameters of the distribution come afterwards so the location and the scale and all random variables in Aesara follow this format. The first three variables are always random number size,\noutput d type and then the parameters of the distribution in order. Okay, so we'll just compile a function and take as input the location output, the normal and we'll evaluate it with a location of zero we'll get a random draw I'll call it again and i'm going to get the same value\nbecause the inputs are exactly the same, that might be a bit surprising but all\nAesara graphs or functions they are deterministic operations or deterministic functions of the inputs so if you don't change the inputs you get the same output\nand in the case of random variables one of the inputs that's very relevant is this random number generator which tells the state of your random number generator and if you want a different value you have to update these.\nSo we can in this case just call set value because this was a shared variable so\nit's a constant we can mutate so we just set it to a new random number generator with a\ndifferent seed and now if we call it we'll get a different value for the function. Okay, again it's going to be the same across the evaluations so what happens in Aesara when you are taking random draws is, every time you call the function you are updating the random number generator behind the scene so that you get different draws in every call and this is actually exactly what numpy does if you are more familiar with our number random number generation works in numpy, Now this is obviously a bit cumbersome for users so there are utilities both in Aesara and also in pymc to make our lives easier.\n\n00:24:05 PyMC utility to create an Aesara function that updates seeds automatically\n\nSo in pymc you can use from the Aesara f module you can use a handy compile pymc, which is just a wrapper around the desired function so you pass inputs outputs you can pass a random seed and it will make sure to create a function that every time you call it will update all the random seeds that are present in the function and now you can call it multiple times and you'll always get different values.\n\n00:24:40 Question break\n\nOkay I will take just if there's one or two questions, I will take them quickly before I go to the next section. I've mainly been answering questions in chat so yeah but like if someone has one maybe just unmute yourself and then you can ask but otherwise I think we can continue.\nI think you may need to raise your hand and then you can be unmuted.\nOkay, so I will continue feel free to pass in the chat and I will collect the\nquestions in the next break or I see that some people have been answering that's also\na good way to get your replies.\n\n00:25:48 PyMC (random side); How PyMC uses Aesara functionality to do prior predictive draws from a model & taking posterior predictive draws from the same model\n\nSo now that you have a crash course in Aesara and the random number generated in Aesara. I'll just show you how pymc makes use of these functionalities to do two of the\nmost important operations in the bayesian workflow, which are taking prior predictive draws\nfrom a model and doing posterior predictive draws from the same model.\n\n00:25:48 PyMC distributions are just functions that return random variables\n\nSo the first thing to know about PyMC is that these things we call distributions in PyMC are just functions that return random variables. If you call pmnormal.dist so just I can create a normal random variable outside of a PyMC context, what you get back is the familiar normal random variable with the same exactly same type of inputs, again you can compile it and you can call it several times to get random draws and if you call it inside the model context it's exactly the same except we also register some meta information in the model object. So in this case we don't call the dot dist, we just call pm.normal we give it a name mu tau, some size and again its output is still just going to be a random variable which has different inputs, for instance a size of two okay. I don't know why I had x.eval, I will take it and I'll show you okay.\n\n00:26:52 Using the helper function pm.draw to debug models\n\nSo like every Aesara variable you saw, you can call a vowel on it in this case it takes no extra inputs because all the inputs were constant and of course if you call it multiple times it's always going to be the same because no input was changed including the random number generator. We could compile a function to just take\nrandom draws but because this is so useful and handy we just have a helper in PyMC called pm.draw you pass any variables, you specify how many draws you want and it will take care of compiling the function and calling the given number of times and also concatenates the outputs I think. So this is a handy way if you just want to debug models, just put the variables, take draws and see if they have like the right shape or values that seem reasonable.\n\n00:27:35 A PyMC model is just an Aesara graph composed of multiple RandomVariables\n\nAnd then PyMC model is basically this, so when you create PyMC model you're just creating an Aesara graph that usually has multiple random variables and some also tensor operators on random variables, so this will be a graph that I will be reusing multiple times during this talk\nso let me just present it. We'll define a new variable that follows a normal distribution. a unit normal then a log sigma variable that also follows a unit normal and then we'll have a likelihood it's a y variable that follows a normal whose mean is the mu and sigma is the exponent of log sigma and we say that we observe the values four five six,okay. And in plot notation this looks like this and  if we just output y we'll see that this looks if we just dprint y we'll see it's just a graph, Aesara graph made with three random variables the output is our y variable whereas as inputs for the location mu which is itself a random variable, a normal random variable and for the scale it takes the exponent of the log sigma okay. All these variables because they were created in a model context are stored so in the model dot free rvs you get the random variables that are not observed are mu and log sigma, in model observed  rvs you get the likelihood y and the basic rvs contains both free random variables and observed\nrandom variables, okay.\n\n00:29:06 Prior predictive sampling with PyMC models\nSo at this step we are ready to make prior, to do prior predictive sampling and the posterior predictive sampling with PyMC models. Prior predictive sampling is very simple, we just compile again PyMC function. We compile a function that will take care of doing the update of the random number generators in prior predictive usually don't have any inputs and outputs will be all the random variables in your model.\nThe three random variables will be the prior and the observe will be the prior predictive\nbut usually we use a single function to take both rows. We compile the function we can call it\ntwice and we are getting draws from the prior predictive, prior predictive draws from the model.\nAnd this is not very different from what happens when you call pm dot sample price predictive\nexcept some more massaging of the output goes on so that for instance, if you call it by default you get a dictionary with the values concatenated for all the variables and if you call with inference data equals true you get an infrastate object with your prior draws mu sigma and in prior predictive you have y but behind the scenes not much more is happening than creating this function, right.\n\n00:30:32 Posterior predictive modeling with PyMC models\n\nPosterior predictive, it's almost as easy except this will be a function whose outputs are usually observed in the variables only and the inputs, we actually will pass values in place of\nthe free random variables those that we sample during you know posterior sampling.\nThis is very similar to in the example above when I show that you could create a function\nthat has as inputs intermediate nodes so when we had a function of y in terms of z instead of x and y you can think here that we have a function of the random variable y not in terms of\nthe random draws of x and wait what was the mu and log sigma, yeah but it'll be some values that we input into the function that have the same shape as a mu and log sigma would. So we create that function, we'll just create some dummy values let's say we took two draws from the posterior and mu was five in the first draw the first sample and twelve in the\nsecond and log sigma was minus two and then one and positive predictive is just basically\nlooping through the values and then calling the function with these values as inputs once at a time.And here we have two possible predictive draws for y and this is very similar to what you would get when you do this except instead of manually specifying the values we usually sample them.\n\n00:32:03 Question break\n\nAll right. So I'll make another break if there's new questions before I jump to the next section.\nYeah there's a lot of discussion in the chat so you're good.\n\n00:32:35 AePPL\n\nAll right AePPL is a new library in the ecosystem of PyMC and Aesara, ae is the prefix we are using for all the Aesara-like libraries so this can be read as Aesara probabilistic programming language and this is a new library that i'm really excited to to share with you who has a very narrow goal which is, convert graphs Aesara graphs made of random variables into log probability graphs . I\u2019ll go a bit what this means and also what it can do\nin part how to say that, so the cool thing about AePPL is that you can define graphs that are not made just of raw random variables but you can actually condition on some operations on random variables, I'll show examples of that and these include vanilla tensor operations like exponentiation, logarithm, multiplication, indexing, conditionals loops.\nIt supports variable transformations which are almost completely critical for net sampling for instance, and it also has it does some nice latex output of random variable graphs or low probability graphs once they are converted.\n\n00:34:00 AePPL crash course: Converting Random Variable graphs to log probability graphs\n\nSo crash course in AePPL, we\u2019ll create the most basic random variable graph with just\na random normal, a unit random normal and we'll ask AePPL to give us the log\nprobability of the random variable of this x rv evaluated at zero okay. The way we call is AePPL.jointlogprob and we pass a dictionary of random variable 2, where we want to evaluate\nthe density or the log probability of this random variable and we get better back a\nverbose graph that just corresponds to basically the density of a normal distribution with\nsome checks to make sure it's mathematically valid, and we can just evaluate and we get what we expect when we evaluate the unit normal distribution at zero, just like we can do here with Scipy. More common we don't want to just have a function of constant value but something we can change across multiple evaluations so we just say I want to evaluate the density of this variable at an input variable input scalar variable, we just do exactly the same except now our it will be a function of this x vv, v for value variable which we can change and evaluate at different\npoints which would look something like this.\nThen AePPL can do obviously not just a single random variable but it can do the graph of multiple random variables that's why we call it joint log prob and so here is our graph that we already saw in PyMC before with our mu log sigma y but now all built directly with two Aesara  random variables and we'll specify that our mu and log sigma will be variables that we pass as input they'll be scalars but our y will be a constant so this is the observed variable so I always want it to be observed at 3 4 5.\nWe call exactly the same function but the dictionary now includes all the three mappings of random variable to a point of evaluation. I will, I guess I can print a graph, is a long\ngraph that has the three terms it just computes the log probability and then adds the terms and then adds the summation of those terms okay. So we'll make a function of this joint log prob graph, the inputs will be the mu vary variable and the log sigma variable the output will be this combined scalar log p and when we evaluate it we'll get a number and we can manually\nwrite it ourselves using Scipy functions just to make sure it's outputting the right results.\nAnd that's the main feature of AePPL.\n\n00:37:00 Create log-probability graphs conditioned on (arbitrary) operations\n\nAnd then where the the power of the library comes is that you can define graphs that are not just a mapping of a random variable to some input but this can be an arbitrary expression that built on top of random variables and not\na random variable directly only. So the simplest case is is this one perhaps,\nwe have a random normal variable let's call log x rv we take the exponent of that\nand we'll actually ask AePPL to give us the joint log probability of this exponent of a variable evaluated at a given point. For those that are familiar with distributions and probability, this graph corresponds to what's known as a log normal distribution and if AePPL did the right thing we should be getting the same output as calling Scipy stats log norm and that's exactly what happens. The cool thing here is that when we said when we create a graph that's an exponent of a variable, it didn't need to be a random normal could have been any other variable and you could just have I don't know beta takes maybe, two values okay, so we can equally easily create a log beta distribution not just things that are pre-built because the rules of how density flows through some deterministic operations are simple to write down. As a more strange case perhaps in this second example, I will create a vector of 10 unit random variables and I will take the cumulative sum of them and I will ask for them. I can also ask for the probability of this cumulative sum evaluated at a given value.\nNot well much but you can just notice that I pass as the value will be just 0 to 9\nand the AePPL graph when evaluated for this value gives the value given for the initial point and then all the others are just as if you had a normal variable evaluated at one okay, and this is exactly what the probability of a cumulative sum evaluated at these points would be like. So we get the first value just at zero and then all the others were just a difference of one so we are equivalent to evaluating the normal at one. We'll go one step further and also show you that we can ask the probability of not only tensor operations but also or not only these basic functions like exponentiation log scaling but also a bit more funky operations like concatenating two vectors of random variables.\nSo I'll create a random variable of size three unit one and then one with a slightly\nlarger signal of size four I will concatenate them and then I'll ask AePPL to give me a function that gives the log probability of this stack of two random variables and when I evaluate what we get is that it figures out that the first three values they should be evaluated at a normal, a unit normal and the last four values should be evaluated at a normal with a scale of two. So this is just you can think of this that we created a new distribution that is just a concatenation of different pure let's say random variables and we can evaluate the probability of this.\nAnd then I'll go a bit, I'll keep digging here sorry if I lost you folks, you can ask them to revisit this example if it was too fast so what i'm going to do is just show that you can combine these different operator kind of operations and nesting and your AePPL will still be able to give you\na density for almost an arbitrary graph. So I will create an initial random variable which is just a log normal again an exponentiation of a normal, I will create this innovation random variable so vector of nine unit variables. I will concatenate the initial random variable with the innovations I just added one dimension and then I will take the cumulative sum. And for those that are familiar with time series this graph corresponds to a Gaussian random log where the initial point follows a different distribution and then we have innovations that all follow the same distribution. And I can evaluate and this is very similar to, this is exactly the same you would get if you use PyMC Gaussian random log with a given init distribution and the given mu and sigma and evaluated values 1 to 10 you get the probability back.\nAnd if it's not very obvious the cool thing is that nowhere did we have to tell what is the probability of any of these operations AePPL was able to figure it out and just give you back the graph when you asked it here.\nAnd I think this is the last example we just we can do this magic for multiple variables at a time so I will create another Gaussian random graph where the drift without an initial point but we'll have a drift parameter which is a negative exponential so we just multiply it by -1 and then we just take a cumulative sum of the ten normals with that drift and one for the scale\nand now we have two variables, so we'll have a scalar input for the drift random variable and a vector input for the Gaussian random log and we'll use a different function that's more core to AePPL which is factorized joint log prob instead of just a joint log prob ,which gives you pass the same input a dictionary that says for each random variable or for each variable that you want the density for what will be the inputs and it gives you back a dictionary which has keys are the value variables the drift v and Gaussian rumble will be and the keys are the graphs that represent the probability of these variables. So just create a function that takes as inputs this drift and Gaussian random walk and the outputs will be just the values of this dictionary okay.\nAnd you can just evaluate it drift 0 with always the same values for the Gaussian random walk and we get that the log probability of this negative exponential evaluated at zero which is zero and the probability of this Gaussian or this cumulative sum with this drift evaluated at one to ten.\nHere I just change the derivative to b minus one so we have now an exponential that allows for negative values and this is just then I just take the negative of the values one to ten so this is just a show how AePPL can give you a graph of log probabilities conditioned on almost arbitrary operations and obviously this is what PyMC will use once we start working with log probabilities.\n\n00:44:30 Question: Is there a PyMC example using Gaussian random Walk with a unique dist argument?\n\nBefore I go, any new questions? Is there a PyMC example using Gaussian random Walk with a unique dist argument? No but I can show in the end. I mean I think the the docstrings of\npm Gaussian random walks should show how to use the init dist it's just a different distribution as input.\n\n00:44:55 Question: Can PyMC now automatically exploit conjugacy during sampling where available?\n\nAnd did I read somewhere that PyMC can now automatically exploit conjugacy during sampling where available. We are not yet doing this we will probably start doing that in the\nnear future, there is another sister library of this ecosystem aemcmc that's working hard on doing exactly that, you give a given model made of random variables and it finds out\nwhether it can sample from the posterior via conjugate sampling or via some specialized gibbs sampling so those things are really in the workings but I'm not going to mention them in this talk.\n\n00:45:38 PyMC: Probability side\n\nAll right, so now we'll see PyMC, how does the probability side of PyMC works now that we understand what AePPL can do and I will just show how it uses AePPL to convert random variable graphs to log probability probability graphs and also uses AePPL to create these so-called distribution factories so that users can create more flexible distributions dynamically without having to create every possible permutation.\n\n00:46:14 How PyMC uses AePPL to transform random variable graphs into log probability graphs\n\nSo yes PyMC uses AePPL behind the scenes to transform this the graph that users\ndefine which as I mentioned is a graph made of random variables into a log probability graph, so here is our old friend model just to remind, this is just an Aesara graph of random variables there's no densities anywhere here but when you when you create when you create\ndistributions inside the model, one of the things we do is for every random variable we create this value variable very similar to how I was doing it here manually which is just the input variable of the same type as the random variable. So for the mu which is a scalar you'll have a scalar input variable for log sigma you\u2019ll also have scalar and for y it will be just a constant vector of 3 for 5 because y was an observed variable.\nIf you want a log p of a model you just call model log p which behind the scenes will just\ncall AePPL basically passing this dictionary as input the graph will now be a graph of\ndensities similar to what we saw before quite long but doing what we expect and we can get the value variables that are not observed from model value variables and with these we can create our function to evaluate the model joint log probability.\nSo we just call our function, we pass this input mu and log sigma variables which are not the random variables even though they have the same name and I can show that.\nIf I just do this you just see it's a graph that you when nothing happens it means there's no operation this is just an input variable mu and log sigma and I think I can also do this perhaps yeah and you can see it's of there are scalar variables that's what the parentheses mean and they are float64 so this is all these are. So we'll use those as the inputs the output\nwill be the model log p that we called here with this helper function which behind the scene just called AePPL and now we have a function of the model joint log probability and because Aesara allows for automatic differentiation, we almost equally easily we can also get the gradients of our model in this case by just calling d log p and I can compile a function of the value variables the gradient of the value variables with respect to the log p and they evaluated\nhere 3 1, I get these or I could also pass the model log p so we have both log p and d log as outputs the first output will be the log p and the second would be the d log p and one reason why you might want to do this is that as I mentioned before Aesara will reuse any shared computations, so if you have any operations that happen in both the log p and the d log p in this function they will just be computed once instead of if you call the function separately for the two outputs okay.\n\n00:49:24 Bayesian inference: Sampling or optimization\n\nOnce you have densities and gradients you can start doing bayesian inference, I will not show you just for the sake of time or you could just but you could easily do a metropolis sampler or if you know how nuts work you could write your own nut sampler, you're just using as inputs or just working with this Aesara function we define but for the sake of time I'll show you a more simple form of let's say bayesian inference which is just finding the maximum aposroriori, so the single point for which the model has the highest posterior probability.\nPyMC obviously has this under the fine map function which finds that for this model condition on y being three four five the maximum auxiliary values for mu are these and for log sigma are these.\nIf you didn't have PyMC or you had to write your own PyMC optimization functions manually\nyou could do it because all you need again is this log p and d log P function so in this case you just use scipy optimization to do basically the same as the find map we just passed the function just wrapping in a lambda because scipy expects, uses a vector and do the function expects separate inputs and we just do negative because we are minimizing instead of maximizing but once we have that again we can just use any routines that are out there to do sampling or to do optimization from a PyMC model.\n\n00:50:45  PyMC uses AePPL is to build new types of distributions\n\nAnd the last way how PyMC uses AePPL is to build new types of distributions.\nI will show an example for how PyMC creates sensor distributions basically online. I will just generate some random data that corresponds to a censoring process. If you are not familiar with censoring you can check some of the they're very good PyMC examples on censoring but the data generating process is basically this.\nWe have a parameter that we'll want to recover which will be the mu true we'll set it to minus 1.5\nwe take random draws from a normal with this mu, sigma of 1 and we take 10 of those draws and then we clip the values to be always between minus one and one so if you've\ngot a minus two point something that becomes a minus one. The data might look something like this and  you'll see that yeah and to then create equivalent model in PyMC we just create a prior distribution for mu we create this y raw using the pm normal dist because we don't actually want to sample or do anything with this raw, it's just an input to pm censored which is a\nit's kind of a factory of distribution so it creates a censored distribution of any input distribution in this case a normal with this specific mu, could be a beta could be anything.  We say that we are truncating between minus one and one and we pass our observations and under the hood this call is just basically returning this graph at dot clip like we had numpy of y raw minus one minus and one, which AePPL knows how to basically derive the probability of this operation of this clipping operation which again corresponds in the statistical framework to censoring.\nSo I'll just create a model, I'll not show you the whole graph but just to prove what I was saying y is a clipping operation, so it's indeed equivalent to this we can do prior predictive.\nThis is our mu looks before, this is how our y looks before we observed any data so it's a normal with an excess of minus ones and ones because all those values get truncated to\nor clipped minus one and one. We can then sample condition on our observed data which should be pretty fast and behind the scenes AePPL gave you the log probability of this graph a\ncondition on this specific likelihood, it's something you could take the gradients with respect to so we are using nuts to sample mu, I think sampling is done work well our posterior is nicely centered around the real mu which was minus 1.5 and we can now do posterior predictive draws and we'll see that after observing data we expect to have much more minus ones and not so many higher values which is just our model condition on data.\n\n00:54:10 AePPL to allow users to define their own arbitrary distributions\n\nAnd finally, we hope to in the future use AePPL to allow users to define their own arbitrary distributions not just the ones that PyMC offers when you call pm.normal or pm.censored but anything that the user knows AePPL can parse or can try to see if it can. I will just give you an example, this is still work in progress so there is no nice api for users to define their own distribution so we'll just do it manually using things that you might not otherwise see. And so in this example I will write a mixture model in PyMC using AePPL to infer what is the probability of a mixture. So we just start by creating an index variable which will be a categorical with this specific probability of coming of getting 0 one or two and we'll this will be of size 10 and then we'll create components we'll just stack three different distributions roughly centered at minus five zero and five to be easy to visualize and then what the mixture corresponds to is basically when you have the components you index it by an integer and this will be basically you take a draw from the right basically component. And we can define this model so this is where we use basically AePPL or we'll use AePPL because AePPL will understand that when you have a stack of distributions and you index by another discrete variable with finite support, it does corresponds to a mixture of the components. So we'll have to manually register the variable because there's no way for users to do it automatically but so we just say I want to register this  mixture variable, I'll give it this name, I'll give it hese dims and I'll just say take a draw from, use the prior for setting an initial point. You can ignore this second point, this is working progress so when you do the plate graph it shows that actually the mixture is deterministic because it doesn't know that we can have variables that are arbitrary operations on pure random variables but it will work well trust me.\nAnd so we don't need that either, let me skip this so I'll just use it to take\ndraws from the prior and this is what the prior of our mix looks like so just these three components with different mixture weights and now we'll do some inference, we'll use the first draw from the prior predictive as our observed data. So this is the values we observed and these were the true indexes that were used in these or meaning that the components from which these values came and so we'll just use exactly the same model but now we'll say the data is the first draw from the prior and I will do inference on the indexes.\nIt should be relatively fast, we are using the categorical gibbs metropolis for our categorical variable, samples pretty pretty fast and we can confirm that our posterior for the indexing nicely matches what the true values were. So this is just to showcase that thanks to AePPL you were able to create a mixture distribution ourselves without PyMC doing anything by just basically creating the graph that when you take draws corresponds to a mixture process and trusting that AePPL knows how to then convert this to a valid log probability.\nAll right, I think that's what I wanted to show so I hope you enjoyed it, I hope you didn't get too lost probably I went a bit too fast so if something was confusing I can\nrevisit the example or if you just have other questions just feel free to ask.\n\n00:58:50 Which composition operators does AePPL not support yet?\n\nJuan has a good question, which composition operators does AePPL not support yet?\nQuite a lot, we support basically multiplication, addition, exponentiation, logarithm and we support some basic support indexing, we support for loops and we support a couple more things like concatenating and stuff. This is a work in progress for instance, I think Larry who is in this joined the talk he's working in for the google summer of code is working on expanding the range of operators that AePPL support and we have nice beginner-friendly issues in the AePPL repo to support more operators so if this is something that you find interesting and you would like to collaborate and expand AePPL, this would be a great way. The ultimate goal is any arbitrary graph for which there's a valid you know there's a valid probability function with respect to it we would like AePPL to support it.\n\n1:00:05 Question: What is the relation between AePPL and Aesara\n\nCarl Talon asks, what is the relation between AePPL and Aesara, I have\nto write an hypergeometric function for a library I'm working on and i'm wondering which\nlibrary would be more appropriate. So before that, AePPL is here I show them side by side but AePPL works on top of Aesara, right so AePPL inputs or Aesara graphs outputs or Aesara graphs it depends exactly what you want to do with hypergeometric. If you want to create a random variable something you can take random draws from that would be pure Aesara code, if you want to define the log probability that could be done with help of AePPL or it might just not need AePPL at all if you just define the function yourself.\nSo the most basic thing that AePPL does is, let me show you the first example, the most basic thing that AePPL does is just if you have a certain random variable of a type you can dispatch a given function that gives the probability of this random variable with the inputs and the value variables so for your hypergeometric you might just have a function that when it sees a similar at.random.hypergeometric or whatever object you created it will know this is the log probability of the distribution.\nWe have in PyMC there are two notebooks you can use to go more in depth on these to understand exactly the code where the things are written of how you can create new random variables and define log probabilities for them in the PyMC docs.\n\n1:02:10 How difficult would it  be to support other data types?\n\nHow difficult would be to support, Sir Robert Mitchell asks how difficult would be to support other data types like polars as an example, could it be a beginner pr?\nSo the way I read that it's, I mean the data is still numeric it's just each whether it's interpreted as a polar data yeah, so the way we do that in PyMC and AePPL is that we use transformations, so we have for instance for the von mises distribution we just transformed whatever input to be on the unit circle. I don't know if it would be a beginner pr but would be good to see if you can implement a distribution that uses polar input values and if you cannot then open an issue and ask on discord to see where the limitations are so that we can unblock that.\n\nThe hypergeometric is a component of a log likelihood function? Sure, perhaps open a topic on our PyMC discourse maybe just exposing you what you had in mind and then I'll be able to give you more proper answer to your question about the hypergeometric and in general any questions about the libraries and what you can do with them the right place will be discourse where we can async give you all the answers. If I missed your question above just feel free to rewrite it or copy paste it, otherwise you can also just raise your hand or join in the chat with voice. Again the notebook is here.\n\n1:04:42 Thank you\n\nWow! Awesome, thank you so much Ricardo for an amazing presentation I definitely learned a lot about how things fit together and most of all thanks everyone for joining for being a part of this community and being interested in how it actually works so yeah. With that thanks and have a great day stay involved in the various links that we posted and hope you'll join us for the next one.",
    "word_count": 10554
  },
  {
    "title": "Bayesian Modeling in Biotech: Using PyMC to Analyze Agricultural Data",
    "url": "https://www.pymc-labs.com/blog-posts/2022-08-11-indigo",
    "date": null,
    "author": null,
    "description": "Uncover the power of Bayesian modeling in biotechnology. Learn how PyMC is used to analyze complex agricultural data, providing valuable insights for the industry.",
    "content": "Bayesian Modeling in Biotech: Using PyMC to Analyze Agricultural Data\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nIntroduction\n\nIn July 2022, we organized a panel discussion with Manu Martinet ofIndigo Agand Thomas Wiecki and Bill Engels of PyMC Labs to discuss a case-study of measuring effects of crop-types in an agricultural setting.  The goal of the project was to identify the underlying spatial pattern and remove it in order to measure more accurately the treatment effect, which in this case are microbes which contribute to plant yield.\n\nPyMC Labs were consultants on this project which had limited data and which used Bayesian analyses and Gaussian processes to identify the treatment effect.  We demonstrate how Bayesian modeling is a powerful tool for solving problems in biotechnology.\n\nTimestamps\n\n00:00Thomas Wiecki does PyMC introduction\n\n02:49Thomas introduces self\n\n03:33Manu Martinet introduces self\n\n04:25Bill Engels introduces self\n\n05:10Panel discussion begins\n\n06:51Testing crop yields on fields\n\n08:16How do you sell the product to farmers?\n\n10:55Data modeling and challenges\n\n13:04Goal of the project: Estimate the spatial pattern and remove it to get the treatment effect\n\n15:20Gaussian processes and how they are used\n\n18:04Spatial Gaussian Processes\n\n19:09Spatial effects\n\n22:13Examples fields to show the spatial components\n\n24:28Question: How does modeling the spatial component with a Guassian process compare with other simpler methods?\n\n25:47Question: With the Gaussian Process(GP)  can you estimate the spatial scale?\n\n28:06Question: How does the Gaussian Process deal with latent variables?\n\n30:08Advantages of the a Bayesian framework\n\n35:00Collaboration between Indigo and PyMC Labs review\n\n42:43Question: What were the biggest challenges in the study?\n\n45:29Question: Is there any example online for PyMC based Hierarchical Gaussian Processes(GP) regression?\n\n46:37Question: How did the decomposition work out between signal, spatial and noise and how do you balance\nthe confidence between what is signal and what is noise?\n\n47:35Question: How to effectively use Bayesian methods to substantiate product claims to regulatory bodies?\n\n48:07Thank you!",
    "word_count": 310
  },
  {
    "title": "Bayesian Item Response Modeling in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/2022-10-26-AlvaLabs",
    "date": null,
    "author": null,
    "description": " Uncover the power of Bayesian Item Response Theory in PyMC. Learn how it revolutionizes data analysis and opens up new possibilities for personality modeling.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian Marketing Mix Models: State of the Art and their Future",
    "url": "https://www.pymc-labs.com/blog-posts/2022-11-11-HelloFresh",
    "date": null,
    "author": null,
    "description": "Learn the cutting-edge of Bayesian Marketing Mix Models and glimpse into their promising future. Uncover how these models are revolutionizing business strategies and decision-making processes.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Hierarchical Bayesian Modeling of Survey Data with Post-stratification",
    "url": "https://www.pymc-labs.com/blog-posts/2022-12-08-Salk",
    "date": null,
    "author": null,
    "description": "Dive into the complexities of Hierarchical Bayesian Modeling applied to survey data with post-stratification. Understand the subtleties of multilevel regression and the potential of Gaussian Processes in this comprehensive analysis.",
    "content": "Hierarchical Bayesian Modeling of Survey Data with Post-stratification\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nIntroduction\n\nIn this panel discussion, Tarmo J\u00fcristo tells us how Bayesian modeling can help in environments where data are noisy and uncertainty is high \u2013like public opinion polls. In particular, data can be sparse in some strata of the population, making the model\u2019s job harder, precisely for the demographics you\u2019re the most interested in.\nA special focus is placed on the work PyMC Labs has done with Tarmo, implementing a state-of-the-art hierarchical Bayesian model. Coupled with post-stratification, this method not only makes inference possible \u2013 it makes it actionable, even you have only a few data points for some demographics.\n\nTimestamps\n\n00:00Introduction by Thomas\n\n03:45Tarmo introduces himself\n\n05:20Panel discussion starts\n\n06:11Description of Salk\n\n08:13Zooming into the data Salk uses\n\n10:04A look into what Tarmo does\n\n13:58Multilevel regression with post-stratification\n\n16:27Further refinements of the Multilevel regression with post-stratification\n\n19:57Model output\n\n25:50Question: On a multilevel aspect, does this mean you model other clusters/groups within other clusters/groups?\n\n28:43Input to simulation\n\n32:20Final simulation\n\n34:46Alex Andorra introduces himself\n\n36:40Question: How do you choose whether it makes sense to add interactions to a model and do you start with all possible interactions?\n\n38:56Technical difficulties during the project\n\n46:59Demonstration of the dashboard\n\n51:52You can use geospatial covariation to extend the model\n\n53:27Does the forecasting take the difference in policies between parties\n\n54:19Using Gaussian Processes in the model(Advantages and disadvantages)\n\n59:55Question: If you have more time, what would you add to the model\n\n1:02:56Question: How well do you think the model is taking without rare events?\n\n1:06:57Thank you!",
    "word_count": 263
  },
  {
    "title": "Likelihood Approximations for Cognitive Modeling with PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/2023-01-12-Akili",
    "date": null,
    "author": null,
    "description": " Dive into cognitive modeling with PyMC. Learn about the impact of likelihood approximations.",
    "content": "Likelihood Approximations for Cognitive Modeling with PyMC\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nIntroduction\n\nDigital therapeutics are evidence-based, clinically evaluated software and devices that can be used to treat an array of diseases and disorders, according to the Digital Therapeutics Alliance, the industry's trade association. They can be used independently or with medications, devices, and other therapies to treat physical and behavioral health conditions, including pain, diabetes, anxiety, post-traumatic stress disorder, and asthma.\n\nIn this talk,PyMC LabsandAkilidiscuss using Bayesian methods and PyMC to test a range of computational models of cognition, specifically with an eye towards ADHD (Attention-deficit/hyperactivity disorder). They focus on some technical challenges and how ideas from likelihood-free inference and machine learning can help overcome them.\n\nTimestamps\n\n00:00Thomas Wiecki introduction\n\n01:27Alex introduction\n\n01:51Titi introduction\n\n02:55Andy introduction\n\n03:42Akili background\n\n05:11EndeavorRx by Akili and PyMC's involvement\n\n09:49Likelihood approximation networks in PyMC\n\n10:15NeuroRacer\n\n15:44Two important aspects of the Model\n\n20:39Inference with model variants\n\n21:05Inference from access to simulators\n\n21:55Inference with models\n\n22:56Training\n\n24:06Previous toolbox: HDDM\n\n24:59Properties inherited from Neural Networks\n\n26:31Graphical representation of model in PyMC\n\n29:42Code in PyMC\n\n30:07Neural network (LAN, CPN)\n\n31:17Proof of concept (Parameter Recovery)\n\n31:42Proof of concept (Speed)\n\n32:41Thomas question on speed\n\n36:11Thomas on Before PyMC vs after PyMC\n\n36:32Titi on before PyMC vs after PyMC\n\n38:11Andy on production use case\n\n39:00Thomas question on application\n\n39:16Andy explains the use case and the application\n\n40:28Titi on impact in applications\n\n41:15Thomas on Knowledge transfer to Akili research team and collaboration\n\n43:02Andy on working with PyMC team\n\n44:55Thomas question to Alex on applying this method to other applications across industries\n\n47:15Why does Akili care about these kinds of models ?\n\n49:31PyMC's work and impact towards Akili's mission\n\n51:04Audience Q/A (What other conditions can this be applied other than ADHD?)\n\n52:03Audience Q/A (Is Data enough to conduct experiments ?)\n\n56:32Closed form solution vs Neural Networks\n\n56:52Optimizing LAN for faster forward pass, primary metric and designing networks\n\nResources\n\nAkili Interactive\n\nWhat are digital therapeutics and their use cases?\n\nUnderstanding DTx - Digital Therapeutics Alliance\n\nLikelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience\n\nOverview of Approximate Bayesian Computation",
    "word_count": 349
  },
  {
    "title": "Bayesian Methods in Modern Marketing Analytics",
    "url": "https://www.pymc-labs.com/blog-posts/2023-06-20-juan-marketing-analytics",
    "date": null,
    "author": null,
    "description": "Discover the innovative application of Bayesian methods in the realm of modern marketing analytics. This article offers a fresh perspective on how these advanced techniques are reshaping the landscape of data-driven marketing strategies.",
    "content": "Bayesian Methods in Modern Marketing Analytics\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nIntroduction\n\nBayesian methods have gained significant popularity in modern marketing analytics due to their ability to handle uncertainty, incorporate prior knowledge, and make accurate predictions. Unlike traditional statistical approaches, Bayesian methods provide a flexible framework that enables marketers to make data-driven decisions by combining observed data with prior beliefs or assumptions.\n\nEvent Description\n\nIn this webinar we discuss some of the most crucial topics in marketing analytics: media spend optimization through media mix models and experimentation, and customer lifetime value estimation. We approach these topics from a Bayesian perspective, as it gives us great tools to have better models and more actionable insights. We take this opportunity to describe our join with PyMC Labs in open-sourcing some of these tools in our brand-new pymc-marketing Python packagePyMC Marketing.\n\nTimestamps\n\n00:00Welcome\n\n02:03Webinar starts\n\n02:32Webinar's objective\n\n03:04Outline\n\n04:05Applied Data Science\n\n05:12Bayesian Methods\n\n06:49Geo-Experimentation\n\n08:27Time-Based Regression\n\n10:26Regression model in PyMC\n\n12:04Marketing measurement\n\n13:34Media Transformations (Carryover (Adstock) & Saturation)\n\n15:50Media Mix Model Target\n\n16:24MMM Structure\n\n16:53Media Contribution Estimation\n\n17:13Budget Optimization\n\n18:18PyMC-Marketing\n\n19:25PyMC-Marketing- More MMM Flavours\n\n20:00Customer Lifetime Value (CLV)\n\n21:47Continuous Non-Contractractual CLV\n\n22:57CLV Estimation Strategy\n\n24:31BG/NBD Assumptions\n\n27:14BG/NBD Parameters\n\n27:50BG/NBD Probability of Alive\n\n28:40Gamma-Gamma Model\n\n29:12BG/NBD Hierarchical Models\n\n31:14Causal Inference (Synthetic control)\n\n32:10Causal Inference (Difference-in-Differences and Regression Discontinuity)\n\n32:39Instrumental Variables\n\n34:46Cohort Revenue-Retention Modelling\n\n38:21Retention and Revenue component\n\n41:02Cohort Revenue-Retention Model\n\n42:34Revenue-Retention Predictions\n\n43:11References\n\n44:25Connect with PyMC Labs\n\n44:50Marketing analytics strategy consultation\n\n47:36PyMC Applied Workshop\n\n48:58Q/A There are so many parameters in MMM which are not identifiable ...\n\n53:00Q/A In the MMM how do you encode categorical control variables?\n\n54:10Q/A How to deal with latent variables?\n\n57:34Q/A If you observe the baseline uplift...How do you measure it in a Media mix model...?\n\n59:15Q/A How does it solve the cold start problem?\n\nResources\n\npymc-marketing open source Python package\n\nSlides\n\nBook a Free 30-Minute Consultation\n\nCurious how these tools can be most effectively used in your particular situation? In this free 30-minute strategy consultation with us, we will:\n\nReview your current setup and any potential pain points you might have.\n\nIdentify where there are areas for improvement.\n\nDefine a plan of how Bayesian open-source tools can help you bring your marketing analytics to the next level.\n\nFree strategy consultation:calendly.com/twiecki/bayes\n\nIf calendly slots are full, email:thomas.wiecki@pymc-labs.com",
    "word_count": 378
  },
  {
    "title": "Building an in-house marketing analytics solution",
    "url": "https://www.pymc-labs.com/blog-posts/2023-07-18-niall-In-house-marketing",
    "date": null,
    "author": null,
    "description": "Get a fresh perspective on constructing an in-house marketing analytics solution. This article offers unique insights into the process, highlighting the benefits and challenges of such an endeavor.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Developing Hierarchical Models for Sports Analytics",
    "url": "https://www.pymc-labs.com/blog-posts/2023-09-15-Hierarchical-models-Chris-Fonnesbeck",
    "date": null,
    "author": null,
    "description": "Grasp the intricacies of hierarchical models in the realm of sports analytics. This article presents a comprehensive analysis of these advanced techniques, highlighting their potential in transforming data-driven sports strategies.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Latent Calendar: Modeling Weekly Behavior with Latent Components",
    "url": "https://www.pymc-labs.com/blog-posts/2023-10-27-Latent-calendar-Will",
    "date": null,
    "author": null,
    "description": " We will delve into how Latent Dirichlet Allocation can be applied to discretized calendar events, allowing us to tap into the model's probabilistic origins and its connection to Bayesian principles, offering a wide array of potential applications and insights.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Mastering Marketing Effectiveness: A Comprehensive Guide for Digital Marketers",
    "url": "https://www.pymc-labs.com/blog-posts/2023-19-11-marketing-effectiveness",
    "date": null,
    "author": null,
    "description": "In today's fast-paced digital marketing landscape, it's crucial to master measuring and understanding marketing strategies' effectiveness. This guide covers the importance of marketing effectiveness, explores various evaluation methods, and presents best practices for implementing an effective marketing measurement strategy.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "AI-based Customer Research: Faster & Cheaper Surveys with Synthetic Consumers",
    "url": "https://www.pymc-labs.com/blog-posts/AI-based-Customer-Research",
    "date": null,
    "author": null,
    "description": null,
    "content": "AI-based Customer Research: Faster & Cheaper Surveys with Synthetic Consumers\n\nOctober 22, 2025\n\nByBenjamin F. Maier\n\nConsumer research costs corporations billions annually, yet it still struggles with panel bias, limited scale, and noisy results. What if synthetic consumers powered by large language models(LLMs) could replicate human survey responses with high accuracywhile providing richer qualitative feedback?\n\nThat's exactly what PyMC Labs researchers just proved in anew preprintthat's changing how we think about AI-powered market research.\n\nThe Problem with Asking AI for Numbers\n\nWhen companies first tried using LLMs assynthetic consumers, they hit a wall. Ask LLMs directly for a 1-5 rating about purchase intent given a product concept, and you get unrealistic distributions \u2014 too many \"3s,\" hardly any extreme responses, and patterns that don't match real human behavior.The conventional wisdom?LLMs just aren't reliable survey takers.\n\nOur PyMC Labs teamshowed that'swrong.The problem wasn't the models, it was how we were asking them questions.\n\nThe Breakthrough: Semantic Similarity Rating (SSR)\n\nInstead of forcing LLMs to pick a number, the research team developed a two-step approach:\n\nStep One:Let AI respond naturally in text (like humans actually think about purchase intent)\n\nStep Two:Map that response to a rating distribution on the 1-5 scale using semantic similarity, comparing the AI's statement to reference anchors for each point\n\nThe results? Using 57 real consumer surveys from a leading consumer products company (9,300 human responses),the SSR method achieved:\n\n90% correlation attainment with product ranking in human surveys\n\nMore than 85% distributional similarity to actual survey results\n\nRealistic response patterns that mirror how humans actually rate products\n\nThis isn't just incrementally better, it's the first approach that produces synthetic consumer data reliable enough to guide real product development decisions.\n\nWhat This Means for Business\n\nFor Product Development Teams:You can now screen dozens of concepts with synthetic panels before committing budget to human surveys. Test ideas faster, iterate more, and reserve expensive panel studies for only the most promising candidates.\n\nFor Product Development Teams:You can now screen dozens of concepts with synthetic panels before committing budget to human surveys. Test ideas faster, iterate more, and reserve expensive panel studies for only the most promising candidates.\n\nFor Consumer Insights Leaders:Synthetic consumers don't just replicate ratings, they provide detailed explanations for their scores.\n\nFor Consumer Insights Leaders:Synthetic consumers don't just replicate ratings, they provide detailed explanations for their scores.\n\nFor Research Innovation:The method works without any training data or fine-tuning. It's plug-and-play, preserving compatibility with traditional survey metrics while unlocking qualitative depth that was previously impossible at scale.\n\nFor Research Innovation:The method works without any training data or fine-tuning. It's plug-and-play, preserving compatibility with traditional survey metrics while unlocking qualitative depth that was previously impossible at scale.\n\nPerhaps most importantly:synthetic consumers showedless positivity bias than human panels,producing wider, more discriminative signals between good and mediocre concepts.\n\nPerhaps most importantly:synthetic consumers showedless positivity bias than human panels,producing wider, more discriminative signals between good and mediocre concepts.\n\nThe Technical Foundation That Makes It Work\n\nThe paper demonstrates something fundamental about LLMs: they've absorbed vast amounts of human consumer discourse from their training data. When properly prompted with demographic personas and asked to respond naturally, they can simulate realistic purchase intent patterns -not because they're copying training examples, but because they've learned the underlying patterns of how different people evaluate products.The team tested this rigorously:\n\nCompared LLM architectures (GPT-4o, Gemini 2.0 Flash)\n\nValidated demographic conditioning (age, income, product category all influenced responses realistically)\n\nBenchmarked against supervised ML approaches (which couldn't match LLM performance even with training data)\n\nThis isn't prompt engineering wizardry, it's a fundamental shift in how we should think about eliciting structured information from language models.\n\nAbout the Research Team\n\nThis work comes from PyMC Labs, led by corresponding authorsBenjamin F. MaierandKli Pappas(Colgate-Palmolive), alongside the broader PyMC Labs and Colgate-Palmolive research team includingUlf Aslak,Luca Fiaschi,Nina Rismal,Kemble Fletcher,Christian Luhmann,Robbie DowandThomas Wiecki.\n\nWhat's Next\n\nThis research opens doors well beyond purchase intent:\n\nExtending SSR to other survey items (satisfaction, trust, relevance)\n\nOptimizing reference statement sets for different domains\n\nCombining synthetic and human panels in hybrid research designs\n\nExploring multi-stage pipelines where one LLM generates responses and another calibrates them\n\nThe fundamental insight\u2014 that textual elicitation plus semantic mapping outperforms direct numerical elicitation \u2014 likely applies far beyond consumer research.\n\nRead the Full Preprint\n\nThis blog post covers the highlights, but the complete paper includes:\n\nDetailed methodology and mathematical framework\n\nFull experimental results across all 57 surveys\n\nComparative analysis of different LLM architectures\n\nReference statement design principles\n\nDemographic conditioning experiments\n\nOpen-source Python implementation of the SSR methodology\n\nDownload the full preprint hereto see the methods, experiments, and detailed results.\n\nWant to learn more about synthetic consumers?\n\nCheck out PyMC Labs\u2019 previous work:\n\nCan LLMs play The Price is Right?Can LLMs play The Price is Right?\n\nCan Synthetic Consumers Answer Open-Ended Questions?\n\nHow Realistic Are Synthetic Consumers?\n\nDiscover how PyMC Labs is helping organisations harness the power of synthetic consumers to transform research and decision-making. See what we\u2019re building inour Innovation Lab\u2014 andconnect withus to learn more.",
    "word_count": 831
  },
  {
    "title": "Solving Real-World Business Problems with Bayesian Modeling",
    "url": "https://www.pymc-labs.com/blog-posts/Thomas_PyData_London",
    "date": null,
    "author": null,
    "description": " A practical guide to solving business problems with Bayesian modeling",
    "content": "Solving Real-World Business Problems with Bayesian Modeling\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nIntroduction\n\nAmong Bayesian early adopters, digital marketing is chief. While many industries are embracing Bayesian modeling as a tool to solve some of the most advanced data science problems, marketing is facing unique challenges for which this approach provides elegant solutions. Among these challenges are a decrease in quality data, driven by an increased demand for online privacy and the imminent \"death of the cookie\" which prohibits online tracking. In addition, as more companies are building internal data science teams, there is an increased demand for in-house solutions.\n\nIn this talk Thomas explains how Bayesian modeling addresses these issues by:\n\n(i) Incorporating expert knowledge of the structure as well as about plausible parameter rangers.\n\n(ii) Connecting multiple different data sets to increase circumstantial evidence of latent user features.\n\n(iii) Principled quantification of uncertainty to increase robustness of model fits and interpretation of the results.\n\nInspired by real-world problems we encountered at PyMC Labs, we will look at Media Mix Models for marketing attribution and Customer Lifetime Value models and various hybrids between them.\n\nAbout Speaker\n\nDr. Thomas Wiecki is an author of PyMC, the leading platform for statistical data science. To help businesses solve some of their trickiest data science problems, he assembled a world class team of Bayesian modelers founded PyMC Labs -- the Bayesian consultancy. He did his PhD at Brown University studying cognitive neuroscience.\n\nGitHub:https://github.com/twiecki\n\nTwitter:https://twitter.com/twiecki\n\nWebsite:https://twiecki.io/\n\nTimestamps\n\n00:00Welcome!\n\n0:05Speaker introduction and PyMC 4 release announcement\n\n1:15PyMC Labs- The Bayesian consultancy\n\n2:39Why is marketing so eager to adopt Bayesian solutions\n\n3:49Case Study: Estimating Marketing effectiveness\n\n6:00Estimating Customer Acquisition Cost (CAC) using linear regression\n\n7:36Drawbacks of linear regression in estimating CAC\n\n10:02Blackbox Machine learning and its drawbacks\n\n11:27Bayesian modelling\n\n11:52Advantages of Bayesian modelling\n\n14:12How does Bayesian modelling work?\n\n16:53Solution proposals(priors)\n\n17:26Model structure\n\n19:57Evaluate solutions\n\n20:16Plausible solutions(posterior)\n\n22:36Improving the model\n\n23:38Modelling multiple Marketing Channels\n\n24:51Modelling channel similarities with hierarchy\n\n26:13Allowing CAC to change over time\n\n28:00Hierarchical Time Varying process\n\n30:05Comparing Bayesian Media Mix Models\n\n30:47What-If Scenario Forecasting\n\n31:53Adding other data sources as a way to help improve or inform estimates\n\n33:00When does Bayesian modelling work best?\n\n33:35Intuitive Bayes course\n\n34:38Question 1: Effectiveness of including variables seasonality?\n\n36:03Question 2: What is your recommendation for the best way to choose priors?\n\n38:16Question 3: How to test if an assumption about the data is valid?\n\n39:07Question 4: Do you take the effect of different channels on each other into account?\n\n41:33Thank you!\n\nImportant Links\n\nPyMC Labs\n\nIntuitive bayes course",
    "word_count": 419
  },
  {
    "title": "From Weeks to Minutes: Accelerate building your Bayesian Marketing Mix Model using Fivetran & PyMC-Marketing",
    "url": "https://www.pymc-labs.com/blog-posts/accelerating-bayesian-mmm-fivetran-pymc-marketing",
    "date": null,
    "author": null,
    "description": "Fivetran and PyMC-Marketing integrate to deliver a production-grade Bayesian MMM pipeline. Standardized dbt outputs flow directly into PyMC-Marketing loaders, enabling faster insights, defendable uncertainty estimates, and scalable budget optimization.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Unraveling Cause-and-Effect With AI: A Step Towards Automated Intelligent Causal Discovery",
    "url": "https://www.pymc-labs.com/blog-posts/ai-for-causal-discovery",
    "date": null,
    "author": null,
    "description": null,
    "content": "",
    "word_count": 0
  },
  {
    "title": "Introducing the BETA Release of Our MMM Agent - Powered by PyMC-Marketing",
    "url": "https://www.pymc-labs.com/blog-posts/ai-mmm-agent-beta",
    "date": null,
    "author": null,
    "description": "We're thrilled to open up BETA access to the latest version of our Marketing Mix Modeling (MMM) Agent - a fully AI-driven assistant built on top of PyMC-Marketing that turns what used to be a multi-month modeling effort into an interactive, informative, and insightful workflow in hours.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayes is slow? Speeding up HelloFresh's Bayesian AB tests by 60x",
    "url": "https://www.pymc-labs.com/blog-posts/bayes-is-slow-speeding-up-hellofreshs-bayesian-ab-tests-by-60x",
    "date": null,
    "author": null,
    "description": "How to massively speed up inference for AB tests",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Application of Bayesian Computation in Finance",
    "url": "https://www.pymc-labs.com/blog-posts/bayesian-computation-in-finance",
    "date": null,
    "author": null,
    "description": "This blog post explores the transformative potential of Bayesian computation and PyMC in the field of finance. It highlights how Bayesian methods can enhance financial analysis by quantifying uncertainty, overcoming restrictive assumptions of traditional econometric models, and managing complex model structures with non-normal distributions. The post emphasizes the advantages of Bayesian statistics, such as its ability to provide a comprehensive and interpretable framework for decision-making under risk, and showcases practical applications using PyMC.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian inference at scale: Running A/B tests with millions of observations",
    "url": "https://www.pymc-labs.com/blog-posts/bayesian-inference-at-scale-running-ab-tests-with-millions-of-observations",
    "date": null,
    "author": null,
    "description": "Industry data scientists are increasingly making the shift over to using Bayesian methods. However, one often cited reason for avoiding this is because \u201cBayesian methods are slow.\u201d ",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian Baseball Monkeys",
    "url": "https://www.pymc-labs.com/blog-posts/bayesian-marcel",
    "date": null,
    "author": null,
    "description": "Using Bayesian methods to implement a MARCEL-style projection system for MLB ...",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian Media Mix Modeling for Marketing Optimization",
    "url": "https://www.pymc-labs.com/blog-posts/bayesian-media-mix-modeling-for-marketing-optimization",
    "date": null,
    "author": null,
    "description": "Learn about Bayesian Media Mix Modeling",
    "content": "Bayesian Media Mix Modeling for Marketing Optimization\n\nSeptember 23, 2025\n\nByBenjamin Vincent\n\nA problem faced by many companies is how to allocate marketing budgets across different media channels. For example, how should funds be allocated across TV, radio, social media, direct mail, or daily deals?\n\nOne approach might be to use heuristics, i.e. sensible rules of thumb, about what might be most appropriate for your company. For instance, a widely used approach is to simply set your marketing budget as a percentage of expected revenues. But this involves guesswork - something we want to avoid regardless of the size of the marketing budget involved.\n\nFortunately, with Bayesian modeling, we can do better than this! So-calledMedia Mix Modeling(MMM) can estimate how effective each advertising channel is in gaining new customers. Once we have estimated each channel\u2019s effectiveness we can optimize our budget allocation to maximize customer acquisition and sales.\n\nWe at PyMC Labs had an opportunity to unleash our skills on Bayesian MMM\u2019s in partnership withHelloFresh(check theircareers page). Our collaboration started after HelloFreshpresented their Bayesian MMMatPyMCCon 2020(make sure to check out theirMMM blog post). Out of the natural post-presentation discussion, we decided to work together to see how the PyMC Labs team could build upon the great work by HelloFresh\u2019s data science team.\n\nIn this blog post, we outline what you can do with MMM\u2019s, introduce how they work, summarise some of the benefits they can provide, as well as covering some of the modeling challenges. In a follow-up blog post, we will discuss the specific model improvements we implemented.\n\nWhat is the Bayesian Media Mix Model?\n\nA Bayesian Media Mix Model (MMM) is a sophisticated statistical approach used to analyze and optimize the effectiveness of various marketing channels in driving customer acquisition and sales. This model applies Bayesian inference to understand the relationship between marketing activities and key business metrics, such as customer acquisition or sales volume.\n\nKey Concepts of Bayesian MMM\n\nAt its core, a Bayesian Media Mix Model combines historical marketing data with prior knowledge to estimate the impact of different media channels. Unlike traditional regression models, which provide point estimates, a Bayesian MMM offers a probabilistic framework. This allows for the incorporation of prior beliefs or expert knowledge and updates these beliefs as new data becomes available.\n\nBayesian Inference: Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability estimate for a hypothesis as more evidence or information becomes available. In the context of a Bayesian MMM, it means continuously refining the estimates of how different media channels contribute to customer acquisition based on new data.\n\nMedia Channels and Their Impact: The model evaluates various media channels\u2014such as TV, radio, social media, and direct mail\u2014by analyzing how changes in spending across these channels affect key performance indicators. This involves estimating coefficients that represent the effectiveness of each channel, which can be adjusted as new data is incorporated.\n\nIncorporation of Prior Knowledge: One of the unique features of Bayesian MMM is its ability to integrate prior knowledge into the modeling process. This could include insights from past marketing campaigns, industry benchmarks, or expert opinions. By combining this prior knowledge with data-driven insights, the Bayesian approach helps in creating a more robust model.\n\nProbabilistic Estimates and Uncertainty: Unlike frequentist approaches that provide single-point estimates, Bayesian MMM provides a range of probable outcomes for each parameter, reflecting the uncertainty inherent in the data. This means that businesses can understand not just the likely effectiveness of each channel but also the degree of confidence in these estimates.\n\nDynamic and Iterative Learning: Bayesian Media Mix Models are not static; they evolve over time as new data is collected. This dynamic nature allows the model to adapt to changes in market conditions, consumer behavior, and marketing strategies, providing updated insights and recommendations continuously.\n\nWhat can you do with Media Mix Modeling?\n\nMedia Mix Modeling gives rich insights and is used in many ways, but here are some of the highlights:\n\nUnderstand the effectiveness of different media channels in driving customer acquisition.Not only can you learn from data about the most influential media channels for your business, but you can update this understanding over time. By incorporating new marketing and customer acquisition data on an ongoing basis, you can learn about the changing effectiveness of each channel over time.\n\nAvoid being misled by other factors.If the rate of customer acquisitions dramatically changes, was this caused by changes in marketing spend across media channels? Or was it caused by other factors such as changes in seasonality, consumer sentiment, economic factors, pricing changes, etc.?\n\nInform media spending decisions.Having gained an understanding of the effectiveness of different media channels, such as knowing the customer acquisition cost per channel or the degree of channel saturation, this could be used to inform future marketing spend across channels.\n\nOptimize future marketing decisions.Rather than just inform future budget spending decisions, it is actually possible to optimize these spending decisions. For example, it is possible to calculate budgets across media channels that maximize new customers for a given total budget. Seethis blog post on Bayesian decision-makingfor more information.\n\nInspire marketing experiments.If there is uncertainty about the effectiveness or saturation of channels, we can intelligently respond to this by running lift or incrementality tests to resolve some of this uncertainty.\n\nValidate your understanding through predictions.We gain confidence in our knowledge of the world by making predictions and comparing them to what happens. MMM also generates forecasts that we can check against reality. As a result, we can improve our understanding and modeling iteratively to become more accurate over time.\n\nHow does Media Mix Modeling work?\n\nIn simple terms, we can understand MMMs as regression modeling applied to business data. The goal is to estimate the impact of marketing activities and other drivers on a metric of interest, such as the number of new customers per week.\n\nTo do this, we use two main types of predictor variables:\n\nThe level of spend for each media channel over time.\n\nA set of control measurements that could capture seasonality or economic indicators.\n\nThe basic approach to MMMs uses linear regression to estimate a set of coefficients for the relative importance of each of these predictors, but real-world MMMs commonly incorporate also non-linear factors to more accurately capture the effect of marketing activities on consumer behaviour:\n\nThe reach function:Rather than model the number of customers acquired as a linear function of marketing spend, the reach function models the potential saturation of different channels: While the initial money spent on an advertising channel might have a big impact on customer acquisition, further investment will often lead to diminishing returns as people get used to the message. When we think about optimization, modeling this effect is critical. Some channels may be nowhere close to being saturated and yield significant increases in customer acquisitions for spending for that channel. Knowing the saturation of each channel is vital in making future marketing spend decisions.\n\nThe adstock function:The marketing spend for a given channel may have a short-term effect or long-term impact. Remember that jingle from a TV ad you\u2019ve seen 20 years ago? That\u2019s a great long-term impact. The adstock function captures these time-course effects of different advertising channels. Knowing this is crucial - if we know some channels have short-term effects that quickly decay over time, we could plan to do more frequent marketing. But suppose another channel has a long, drawn-out impact on driving customer acquisitions. In that case, it may be more effective to use that channel more infrequently.\n\nThus we can summarize the full MMM with this image:\n\nThe benefits of a Bayesian Media Mix Model\n\nTraditionally, one fits an MMM model using frequentist methods such asOrdinary Least Squares(which often provide a reasonable pointwise estimate for the coefficients) but it is becoming increasingly common to use the Bayesian approach, which was first introduced by Google in 2017 [Jin et al 2017]. Some of the core benefits thatBayesian Media Mix Modelinghave are:\n\nWe can incorporate our prior knowledge.Prior knowledge about marketing channel effectiveness can come from various sources\u2014and we want to be able to use that knowledge. One source could be the intuition that marketing managers have accumulated running hundreds of campaigns, or their knowledge about changes in the price of a competitor\u2019s product, etc. Another source of prior knowledge could be the results of carefully conducted field experiments such as lift or incrementality tests. The Bayesian approach allows prior knowledge to be elegantly incorporated into the model and quantified with the appropriate mathematical distributions.\n\nWe can optimally combine that prior knowledge with data.At the heart of the Bayesian approach is how we update our knowledge in the light of new data. Using a Bayesian approach enables you to combine your team\u2019s valuable knowledge with modeling insights from the MMM\u2019s data-driven strategy. No longer will models overrule human knowledge; with Bayesian methods, they augment human knowledge.\n\nWe can know how sure we are.Another significant benefit of the Bayesian approach is that we know how confident we are. Yes, we want to make data-driven decisions. Still, we only want to do so when the data provides us with a sufficient level of confidence in our understanding. Parameters pertaining to Marketing channels that are estimated with high level of uncertainty can motivate targeted marketing efforts (i.e. incrementality tests) to resolve this uncertainty.\n\nDid you spot the recurring theme of uncertainty? If we already knew everything and had no uncertainty then our job would already be done. But the Bayesian approach offers a principled way of describing, dealing with, and reducing our uncertainty based upon data.\n\nChallenges with Bayesian MMM\u2019s\n\nIn one way, MMMs are conceptually simple. So how hard can it be to implement these kinds of models in a Bayesian framework? Well, not so fast! Things can get tricky when moving from concepts to implementation. Here is a list of possible stumbling points\u2014which are precisely the kind of modeling decisions that our team at PyMC Labs can help with:\n\nWhich likelihood function to choose?There are several feasible options here. Suppose we are modeling customer counts. We could consider discrete distributions like thePoisson distributionbut also theNegative binomial distributionwould be a potential option. Alternatively, if the number of customers is very large, we may also choose aNormal distribution; or perhaps a distribution with longer tails, such as theCauchyorStudent\u2019s Tdistribution.\n\nWhat priors to use?Bayesian MMM\u2019s will require priors over numerous parameters, such as regression coefficients on the control measures and the parameters in the reach and adstock functions. It is easy to end up with hundreds of parameters for state-of-the-art models. Some choices of priors may be less appropriate than others and lead to bad convergence problems. And how, exactly, should we use the data from lift tests to improve these priors?\n\nHow should the reach and adstock functions be parameterized?The parametric forms of reach and adstock functions also have many options. Some ways of parameterizing these functions will be more interpretable than others and more readily provide insights to drive decision-making.\n\nHow to select the correct model?In all likelihood, at least during development, dozens of different MMM variations may be considered. How can we rapidly assess which ones are most promising? It may be useful to have a system in place that allows rapid experimentation, allowing different background variables to be brought in and out of the MMM. Having a coherent and easy-to-change setup is very useful in supporting experimentation and evaluation of \u2018what if\u2019 questions.\n\nHow can we ensure rigorous, correct, and fast inference?While Bayesian modeling is very powerful when working with real-world models for decision making it is crucial to ensure the inference is correct. There can be a number of problems that may trap a novice here, such as the correlation between MCMC chains, divergences in sampling, over-specified models\u2026 and even subtle aspects about interpreting the interplay between the model, the priors, and the data.\n\nPyMC Labs services\n\nHere at PyMC Labs, we offer bespoke Bayesian Media Mix Modeling services. For this particular project withHelloFresh(who are currently recruiting) our core contributions were:\n\nReparameterization of the model to make it much more interpretable.\n\nSpeeding up the sampling process and fixing MCMC chain convergence problems, and\n\nBuilding a software framework around the existing code to increase usability and maintainability.\n\nIf you are interested in learning what we did in more detail, then check out thesecond blog postin this series on Bayesian MMM\u2019s.",
    "word_count": 2070
  },
  {
    "title": "Bayesian Vector Autoregression in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/bayesian-vector-autoregression",
    "date": null,
    "author": null,
    "description": "It's time to let go of your custom Gibbs sampler",
    "content": "Bayesian Vector Autoregression in PyMC\n\nSeptember 23, 2025\n\nByRicardo Vieira\n\nA little bit of history\n\nIn the seminal 1993paper\"Bayes regression with autoregressive errors: A Gibbs sampling approach\", Siddhartha Chib presented to economists the potential of Gibbs sampling in \"realistic settings that were previously considered intractable from a Bayesian perspective\".\n\nA cursory reading reveals a pattern that is quite characteristic of this early type of work. A time-series statistical model is first described, usually in a handful of equations \u20141 in this case\u2014, followed by an intricate analysis of how the model joint-probability can be cut into conditionally independent blocks appropriate for Gibbs sampling. In that paper this amounts to no less than 9 equations! If one's model could not be easily cut into sample-able blocks,\"auxiliary\" variablescould always be added as topping, until such cut was found.\n\nAs if this was not hard enough, early pioneers had to write their custom Gibbs sampling algorithms by hand, taking care to accommodate the existing computing limitations. Chib's paper makes mention of a powerful 33MHz machine. This process was somewhat alleviated by increasing compute power and by the advent of general purpose MCMC samplers likeBUGSandJAGS, and later more powerful Hamiltonian Monte Carlo samplers like those found inStanandPyMC.\n\nSurprisingly, many are still relying on custom Gibbs sampler algorithms tothis day. Perhaps begging their joint probabilities to factor nicely or chasing the auxiliary variables needed for their latest model ideas. Needless to say, with PyMC, we won't be doing any of this!\n\nBack to the present\n\nWe will implement a Vector Autoregression (VAR) model, a powerful time series tool, to examine and forecast complex dynamic relationships between variables. VAR models are routinely used by most macroeconomic and policy-making institutions, and have been increasingly adopted in other areas. Some interesting applications include:\n\nTo understand how VAR works, it helps to first take a brief look at the simpler Autoregression models.\n\nAutoregression models\n\nAutoregression models (AR) try to predict current observations as a linear combination of the past observations.\n\nAssuming that the most recentvalues influence the current observation, the model is described by the following equation:\n\nSimulating data from an AR process is quite simple:\n\nIn this first example, new values forat timedepend only on the intercept and the previous value. This would be an autoregression model of first order or AR(1).\n\nIn an AR(2), new values are influenced by the two preceding values.\n\nAs the order of an AR model increases, so does the range of temporal dependencies and the richness of patterns that can be captured. On the other hand, the effect of each coefficient will be harder to determine, as each one plays a proportionally smaller role in the model outcomes.\n\nVector Autoregression models\n\nVector autoregression models (VAR) are the multivariate generalization of AR. In a similar vein, they assume that a set of variables, observed at time, can be well predicted by a linear combination of the previous observations of that same set of variables.\n\nAssuming that only the most recent two values have an effect on the current observations, the VAR(2) model is described by:\n\nThis type of model allows for bidirectional effects among variables. The current values ofare not only affected by its past values, but also by the past values of. In addition, the past values ofalso affect the current values of. This type of bidirectional effects make VAR models extremely flexible and powerful.\n\nLet us simulate data from a VAR(2) model:\n\nUnlike the simulations in the previous section, the two time-series ofandare now interdependent. Concretely,depends onanddepends on bothand. If the coefficients for the cross-effects were all set to zero, these effects would vanish and we would be observing two independent AR processes just like before.\n\nGoing Bayesian with BVAR\n\nBayesian Vector Autoregression models (BVAR), are the Bayesian interpretation of vanilla VAR models. Parameter uncertainty is explicitly modeled and updated via the Bayesian rule, conditioned on observed data. Like most Bayesian models, there are no hidden assumptions or special conditions under which a statistical model can or cannot be used. What youseewrite is what you get.\n\nThis single letter difference gives a lot of power to BVAR models. Model parameters can be directly constrained by expert information. Relevant information from different datasets can be pooled together via hierarchical modelling. Different assumptions about the extent of lagged effects or noise terms can be easily changed and contrasted. It is also quite simple to extend and combine BVAR with other types of models. As with most Bayesian tools, imagination is the only limitation.\n\nWell, that and compute time...\n\nVAR + PyMC = BVAR\n\nThe good news is that if you are using PyMC, your VAR model is necessarily a BVAR model! Let's see how easy it is to do it:\n\nTo honor the economic tradition of VARs, we will look at U.S. macroeconomic data for our example, and modelGDP growthandTerm spreadbetween long- and short-term Treasury bonds. If you have no idea what that means... you are probably not an economist. Neither am I, so do not worry!\n\nThis analysis is heavily inspired by thischapter of Introduction to Econometrics with R. Another useful reading can be found in the respectivechapter of Forecasting: Principles and Practice.\n\nPer usual, we must start with some scraping and data transformation...\n\n132 rows \u00d7 5 columns\n\nTime to model! We will consider a simple BVAR model of second order.\n\nShall we do some armchair economics?\n\nSampling seems to have gone well, let's look at the results:\n\nWe can see GDP growth is positively correlated with growth in the two preceding quarters,[-1, GDPGrowth, GDPGrowth]and[-2, GDPGrowth, GDPGrowth], as the posteriors of both coefficients are positive and far from zero.\n\nTerm spread is strongly correlated with the term spread of the past quarter[-1, Tspread, Tspread]and slightly negatively correlated with that of two quarters past[-2, TSpread, TSpread].\n\nNow for the interesting part of VAR, we want to look at the cross-effects from one variable to the other. The model suggests past Term spreads may be strongly related to  GDP growth, with[-1, GDPGrowth, TSpread],[-2, GDPGrowth, TSpread]having an absolute large mean. However, the uncertainty around these parameters is rather large.\n\nIn contrast, there seems to be almost no effect in the other direction, from GDP growth to term spread[-1, TSpread, GDPGrowth],[-2, TSpread, GDPGrowth], with posteriors of the coefficients confidently close to zero.\n\nEasier to forecast\n\nLooking at numbers can only be so useful. What do they imply? Let's build a helper function to forecast future GDP growth and Term spread, according to our freshly inferred posterior parameters.",
    "word_count": 1082
  },
  {
    "title": "Can LLMs play The Price is Right?",
    "url": "https://www.pymc-labs.com/blog-posts/can-llms-play",
    "date": null,
    "author": null,
    "description": "Synthetic consumers\u2014LLMs simulating human survey participants\u2014are becoming a powerful tool for marketing and behavioral research. They promise faster iteration, lower costs, and broader flexibility than traditional panels. But for them to be useful, they need not only to sound realistic, but also to demonstrate some level of real-world reasoning.\n\nA core question in this space: do LLMs \u201cunderstand\u201d prices?* That is, can they recognize how much everyday items cost, and make decisions based on that understanding?",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Causal analysis with PyMC: Answering 'What If?' with the new do operator",
    "url": "https://www.pymc-labs.com/blog-posts/causal-analysis-with-pymc-answering-what-if-with-the-new-do-operator",
    "date": null,
    "author": null,
    "description": "Learn how to use Bayesian causal analysis with PyMC and the new do operator to answer 'What If?' questions.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "What if? Causal inference through counterfactual reasoning in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/causal-inference-in-pymc",
    "date": null,
    "author": null,
    "description": "Unravel the mysteries of counterfactual reasoning in PyMC and Bayesian inference. This post illuminates how to predict the number of deaths before the onset of COVID-19 and how to forecast the number of deaths if COVID-19 never happened. A must-read for those interested in causal inference!",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Causal sales analytics: Are my sales incremental or cannibalistic?",
    "url": "https://www.pymc-labs.com/blog-posts/causal-sales-analytics-are-my-sales-incremental-or-cannibalistic",
    "date": null,
    "author": null,
    "description": "This post explores causal sales analytics, helping companies estimate whether sales from a new product are incremental or cannibalistic. The article discusses the complexities of such analysis and the need for bespoke causal models, ultimately enhancing decision-making for product portfolio management.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Causal sales analytics: A deep-dive on discrete choice modeling",
    "url": "https://www.pymc-labs.com/blog-posts/causal-sales-analytics-discrete-choice-modeling",
    "date": null,
    "author": null,
    "description": "This post explores how PyMC Labs applied a discrete choice modeling framework to solve a challenging causal sales analytics problem for Colgate-Palmolive. The basic multinomial discrete choice model is more faithful to the underlying data generating process than the interrupted time series model we discussed in a previous blog post. It is also more flexible in that can take into account product attributes such as price and product availability (ACV) and estimates each attribute's impact on sales. However, it fails to capture complex patterns of cannibalization and incrementality. This limitation stems from the model's assumption that unobserved consumer preferences are independent, which leads to unrealistic proportional substitution among all products when a new product is introduced.\n\nTo overcome this, we developed a more advanced solution based on the nested logit discrete choice model. This model organizes products into hierarchical \"nests\" (e.g., by segment or brand) to account for correlations in unobserved consumer preferences within those nests. This allows for a more realistic understanding of how a new product's sales are generated, showing that sales of one product preferentially cannibalize from products in the same nest (i.e., similar products). This innovation of this approach represents a significant advancement in causal sales analytics, providing our client with a powerful tool to make better decisions about product launches and marketing.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "CausalPy - causal inference for quasi-experiments",
    "url": "https://www.pymc-labs.com/blog-posts/causalpy-a-new-package-for-bayesian-causal-inference-for-quasi-experiments",
    "date": null,
    "author": null,
    "description": " Unveil the power of CausalPy, a new open-source Python package that brings Bayesian causal inference to quasi-experiments. Discover how it navigates the challenges of non-randomized treatment allocation, offering a fresh perspective on causal claims in the absence of experimental randomization.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Complete Guide to Cohort Revenue & Retention Analysis: Bayesian Modeling Approach",
    "url": "https://www.pymc-labs.com/blog-posts/cohort-revenue-retention",
    "date": null,
    "author": null,
    "description": "Explore how Bayesian methods can be applied to cohort-level customer lifetime value (CLV) models. This post focuses on combining retention and revenue components for improved forecasting and strategic insights in marketing.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian model to infer private equity returns from capital in and outflows",
    "url": "https://www.pymc-labs.com/blog-posts/everysk",
    "date": null,
    "author": null,
    "description": "Learn how PyMC Labs and Everysk collaborated to develop a Bayesian model that provides insights into private equity returns from capital in and outflows, and how this model differs from standard machine learning analyses.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Announcing the Expert Access Program (EAP)",
    "url": "https://www.pymc-labs.com/blog-posts/expert-access-program",
    "date": null,
    "author": null,
    "description": "Teams face critical deadlines where models must be rigorous, but hiring consultants or adding headcount only solves problems temporarily. This is why we created the Expert Access Program \u2014 to give teams ongoing access to our experts, ensuring their models keep delivering value long after the initial build.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "From Uncertainty to Insight: How Bayesian Data Science Can Transform Your Business",
    "url": "https://www.pymc-labs.com/blog-posts/from-uncertainty-to-insight-how-bayesian-data-science-can-transform-your-business",
    "date": null,
    "author": null,
    "description": null,
    "content": "",
    "word_count": 0
  },
  {
    "title": "Hierarchical Customer Lifetime Value Models",
    "url": "https://www.pymc-labs.com/blog-posts/hierarchical_clv",
    "date": null,
    "author": null,
    "description": "This post explores the application of hierarchical Bayesian models to Customer Lifetime Value (CLV) prediction, improving accuracy for customer cohorts, and addressing seasonality and data sparsity.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "How Realistic Are Synthetic Consumers?",
    "url": "https://www.pymc-labs.com/blog-posts/how-realistic-are-synthetic-consumers",
    "date": null,
    "author": null,
    "description": "Evaluating LLMs on Political and Lifestyle Choices",
    "content": "",
    "word_count": 0
  },
  {
    "title": "AI Innovation Lab: An agentic platform for transforming product development",
    "url": "https://www.pymc-labs.com/blog-posts/innovation-lab",
    "date": null,
    "author": null,
    "description": "At PyMC Labs, we\u2019re tackling a core problem in the CPG industry: product innovation is too slow, fragmented, and disconnected from real consumer needs.\n",
    "content": "AI Innovation Lab: An agentic platform for transforming product development\n\nAugust 20, 2025\n\nByNina Rismal\n\nAbstract\n\nAt PyMC Labs, we\u2019re tackling a core problem in the CPG industry: product innovation is too slow, fragmented, and disconnected from real consumer needs.\n\nTheAI Innovation Labis an end-to-end platform that uses AI-powered synthetic consumers and expert agent collaboration to streamline every stage of product development \u2014 from smarter concept briefs and design iteration to pricing simulation and market validation.\n\nValidated through custom metrics and real-world benchmarks, our U.S. panelreplicates up to 90% of consumer behavior patterns. And we\u2019re just getting started \u2014 this is the foundation of a broader vision: where virtual consumer panel insights can be used to evaluate product characteristics, pricing and even marketing creatives and branding providing quantitative assessments that can be fed as priors in Bayesian models such as Media Mix Models.\n\nReimagining Product Innovation with Synthetic Consumers and Agentic Frameworks\n\nAcross the CPG industry, product innovation is falling behind. It\u2019s too slow, too fragmented, and too costly \u2014 and too often, it\u2019s disconnected from what real consumers actually want.\n\nIn response, a growing number of startups and industry leaders have turned tosynthetic consumers: AI-generated personas built to mirror real-world behaviors, preferences, and decision patterns. The promise? Faster, cheaper feedback \u2014 without the delays and limitations of traditional market research.\n\nBut at PyMC Labs, we asked a harder question:What would it take to not just accelerate product development, but fundamentally rethink it \u2014 combining agentic workflows, multimodal vision models, and validated insights grounded in our expertise in data-driven simulations?\n\nWith one of our major clients in the CPG space, we have developed theAI Innovation Lab\u2014 a new way to turn ideas into market-ready products.\n\nWhere Product Innovation Breaks\n\nDespite billions invested in research, design, and marketing, product innovation in CPG remains flawed. Across every stage, teams face systemic barriers that make it hard to build products real consumers actually want.\n\nProduct decisions are often made without meaningful data:Too often, products are developed in the absence of real insight. Early-stage concepts rely on gut instinct or fragmentary trend reports, especially at smaller or mid-sized CPG companies. Even at the larger players, insights tend to be consolidated late \u2014 pulled from siloed sources, often after key decisions have been made. As a result, product development is guided more by internal alignment than by actual consumer behavior or unmet need.\n\nProduct decisions are often made without meaningful data:Too often, products are developed in the absence of real insight. Early-stage concepts rely on gut instinct or fragmentary trend reports, especially at smaller or mid-sized CPG companies. Even at the larger players, insights tend to be consolidated late \u2014 pulled from siloed sources, often after key decisions have been made. As a result, product development is guided more by internal alignment than by actual consumer behavior or unmet need.\n\nFeedback loops are stuck in silos: In traditional product innovation workflow, each team operates in sequence: product writes the brief, R&D formulates, marketing positions, and research validates \u2014 often weeks or even months later. This rigid, siloed process leaves little room for iteration or collaboration. Once decisions are made, they\u2019re hard to reverse, and by the time feedback arrives, it rarely changes the outcome.\n\nFeedback loops are stuck in silos: In traditional product innovation workflow, each team operates in sequence: product writes the brief, R&D formulates, marketing positions, and research validates \u2014 often weeks or even months later. This rigid, siloed process leaves little room for iteration or collaboration. Once decisions are made, they\u2019re hard to reverse, and by the time feedback arrives, it rarely changes the outcome.\n\nDesign is treated as a downstream detail: Visual identity \u2014 from packaging to typography to color palette \u2014 is often finalized after core product decisions have already been made. By then, there\u2019s little time, budget, or flexibility left to test whether the design actually resonates with consumers. Yet in categories like skincare, beverages, and personal care, packaging is not just a wrapper \u2014 it\u2019s the first interaction, the first impression, and often the deciding factor. Despite this, most teams have no structured way to evaluate how visual choices influence perception, relevance, or shelf appeal.\n\nDesign is treated as a downstream detail: Visual identity \u2014 from packaging to typography to color palette \u2014 is often finalized after core product decisions have already been made. By then, there\u2019s little time, budget, or flexibility left to test whether the design actually resonates with consumers. Yet in categories like skincare, beverages, and personal care, packaging is not just a wrapper \u2014 it\u2019s the first interaction, the first impression, and often the deciding factor. Despite this, most teams have no structured way to evaluate how visual choices influence perception, relevance, or shelf appeal.\n\nPricing relies on guesswork:In most workflows, pricing decisions come late, based on loose benchmarks. It's rarely tested or modeled early, even though price directly shapes demand, positioning, and market size. Without a way to simulate price elasticity up front, teams risk launching too high, too low \u2014 or simply off the mark.\n\nPricing relies on guesswork:In most workflows, pricing decisions come late, based on loose benchmarks. It's rarely tested or modeled early, even though price directly shapes demand, positioning, and market size. Without a way to simulate price elasticity up front, teams risk launching too high, too low \u2014 or simply off the mark.\n\nThe result? A process that\u2019s resistant to change, and misaligned with the consumers it\u2019s meant to serve. Fixing it requires more than speeding things up \u2014 it demands rethinking how products are imagined, evaluated, and brought to life.\n\nReal Value for High-Stakes Decisions\n\nOur solution is designed to bring clarity and confidence to every strategic Product Development choice. From early ideation to go-to-market, it provides data-driven ideation and consumer insights that are both quantitative and actionable. The tools enhances:\n\nBrainstorming: Quickly explore hundreds of concept directions, grounded in trend data and internal insights. Get to better ideas, faster \u2014 with agent-led briefs and early feasibility checks that surface what\u2019s worth pursuing.\n\nBrainstorming: Quickly explore hundreds of concept directions, grounded in trend data and internal insights. Get to better ideas, faster \u2014 with agent-led briefs and early feasibility checks that surface what\u2019s worth pursuing.\n\nProduct Refinement: Test pricing and forecast market potential before launch. Understand how price, positioning, and packaging impact demand \u2014 and align confidently on go-to-market strategy.\n\nProduct Refinement: Test pricing and forecast market potential before launch. Understand how price, positioning, and packaging impact demand \u2014 and align confidently on go-to-market strategy.\n\nKey Capabilities of the AI Innovation Lab\n\nThe AI Innovation Lab is not just a tool \u2014it\u2019s an end-to-end platform to dramatically speed up the product development lifecycle, from initial idea to market validation. The platform includes five main capabilities that streamline and accelerate every stage of product development:\n\nSmarter Product Briefs:The Lab generates product briefs informed by trend data, competitive analysis, and your own historical product performance \u2014 giving teams a clear, evidence-based foundation from day one.\n\nSmarter Product Briefs:The Lab generates product briefs informed by trend data, competitive analysis, and your own historical product performance \u2014 giving teams a clear, evidence-based foundation from day one.\n\nAI Expert Evaluation:Each product brief is reviewed by a panel of AI agents simulating expert perspectives \u2014 covering feasibility, formulation constraints, regulatory risks, and strategic fit. Based on the review, the agents suggest targeted improvements, enabling teams to refine concepts early.\n\nAI Expert Evaluation:Each product brief is reviewed by a panel of AI agents simulating expert perspectives \u2014 covering feasibility, formulation constraints, regulatory risks, and strategic fit. Based on the review, the agents suggest targeted improvements, enabling teams to refine concepts early.\n\nDesign refinement:Product visuals are iterated using advanced multimodal models. Teams can explore and refine images, colors, typography, and claims \u2014 making design an integral part of the development process, not an afterthought.\n\nDesign refinement:Product visuals are iterated using advanced multimodal models. Teams can explore and refine images, colors, typography, and claims \u2014 making design an integral part of the development process, not an afterthought.\n\nSynthetic Consumer Testing:Access feedback in minutes, not weeks. The platform taps into synthetic consumer panels built to reflect real-world demographics and behavioral patterns \u2014 allowing rapid testing of uniqueness, appeal, relevance and purchasing consideration.\n\nSynthetic Consumer Testing:Access feedback in minutes, not weeks. The platform taps into synthetic consumer panels built to reflect real-world demographics and behavioral patterns \u2014 allowing rapid testing of uniqueness, appeal, relevance and purchasing consideration.\n\nMarket Simulation: Forecast market impact before launch. Tools for price sensitivity analysis, market sizing, and competitive dynamics help teams make data-driven decisions about positioning, pricing, and portfolio strategy.\n\nMarket Simulation: Forecast market impact before launch. Tools for price sensitivity analysis, market sizing, and competitive dynamics help teams make data-driven decisions about positioning, pricing, and portfolio strategy.\n\nTogether, these capabilities form a unified system that helps teams move from idea to market with greater speed, precision, and confidence.\n\nFrom Prototype to Proof: Validating the Platform\n\nYou might ask: this all sounds promising \u2014 but how do I know it really works in practice?\n\nThat's why we've validated our platform through real-world testing:\n\nLooking Ahead: One Integrated System\n\nAt PyMC Labs, we\u2019re extending the AI Innovation Lab in two key directions:\n\nCreative Testing\u2013 expanding the Lab\u2019s capabilities to evaluate ads and campaign assets early in the development cycle.\n\nCreative Testing\u2013 expanding the Lab\u2019s capabilities to evaluate ads and campaign assets early in the development cycle.\n\nMarketing Integration\u2013 linking it with ourMMM Insight Agent,built on top of thePyMC-Marketing toolbox, to optimize media spend with precision and transparency.\n\nMarketing Integration\u2013 linking it with ourMMM Insight Agent,built on top of thePyMC-Marketing toolbox, to optimize media spend with precision and transparency.\n\nTogether, these systems form a unified, iterative workflow:\n\nWhat should we launch?The AI Innovation Lab supports product and creative development \u2014 testing concepts, messaging, and packaging with synthetic consumers.\n\nWhat should we launch?The AI Innovation Lab supports product and creative development \u2014 testing concepts, messaging, and packaging with synthetic consumers.\n\nHow should we market it?TheMMM Insight Agentsimulates the impact of different budget allocations, creatives, and channels \u2014 helping teams maximize return on spend.\n\nHow should we market it?TheMMM Insight Agentsimulates the impact of different budget allocations, creatives, and channels \u2014 helping teams maximize return on spend.\n\nThe real power comes from closing the loop. Synthetic panels can help inform priors for Media Mix Model campaign coefficients, while in turn underperforming creatives or channels feed back into the Innovation Lab for refinement, creating a continuous cycle of testing, optimization, and learning.\n\nBy breaking down silos between consumer research and marketing, we help brands align what they build with how they sell \u2014 unlocking smarter product decisions and more efficient growth. Marketing is already PyMC Labs\u2019 strength \u2014 and now we\u2019re extending that intelligence across the full innovation lifecycle.\n\nReady to Accelerate Product Innovation?\n\nThe AI Innovation Lab could be the breakthrough your team needs \u2014 combining synthetic consumers, agentic workflows, and multimodal vision models. Contact us today to explore how this end-to-end platform can help you make smarter, faster product decisions with confidence.",
    "word_count": 1846
  },
  {
    "title": "How to use JAX ODEs and Neural Networks in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/jax-functions-in-pymc-3-quick-examples",
    "date": null,
    "author": null,
    "description": "Learn how to seamlessly integrate JAX-based ODE solvers and neural networks with PyMC for Bayesian modeling.",
    "content": "How to use JAX ODEs and Neural Networks in PyMC\n\nSeptember 23, 2025\n\nByRicardo Vieira and Adrian Seyboldt\n\nPyMC strength comes from its expressiveness.\nIf you have a data-generating process and want to infer parameters of interest,\nall you need to do is write it down, choose some priors and let it sample.\n\nSometimes this is easier said than done, especially the \"write it down\" part.\nWith Python's rich ecosystem,\nit's often the case that you already have a generative function,\nbut it's written in another framework, and you would like to use it in PyMC.\nThanks to the highly composable nature of the PyMC backend, this is simple.\nEven simpler if that framework can also provide you gradients for free!\n\nIn this blog post,\nwe show how you can reuse code from another popular auto-diff framework,\nJAX, directly in PyMC.\n\nWe will start with a dummy example by simply wrapping\na pure function that already exists underpymc.math,\nand then show two real examples:\nreusing an ODE Solver from the Diffrax library\nand a CNN from the Flax library.\n\nThis blog post won't explain in detail why we do things the way they are shown,\nbut will only show you how to do it.\nIf you want to have a better understanding, you should check the PyMC exampleHow to wrap a JAX function for use in PyMCand the relevant PyTensordocumentation ofOp.\n\nWithout further ado, let's import some stuff.\n\nWrapping a pure JAX function\n\nIn this first example,\nwe will wrap thejax.numpy.expfunction so you can use it in PyMC models.\nThis is purely demonstrative, as you could usepymc.math.exp.\n\nWe first create a function that encapsulates the operation (or series of operations)\nthat we care about.\nWe also save the jitted function into a variable.\n\nJAX'sjitfunction accepts a function and returns a function,\nmeaning that we can call onjitted_custom_op_jax:\n\nWe then create the function that computes\nthe vector-jacobian product (vjp) needed for PyTensor gradients.\nJAXvjptakes as inputs a computational graph, expressed as a function, and its inputs.\nIt returns the evaluated graph, which we don't need,\nand a partial function that computes the vjp,\ngiven the output gradients,\nwhich is what we need for PyTensor.\n\nNow for the meaty part!\nWe create two PyTensor Ops,\none for the operation we care about\nand another for the vjp of that operation.\nThis is how we can glue external code into PyMC's backend.\n(Note here: It's a bit verbose, but nothing too complicated.)\n\nHere's what's happening below.\nWe subclass from theOpclass\nand implement 3 methods:make_node,performandgrad.\nFor the vjp we need to implement only the first two.\n\nHow do we know that we've implemented theOps correctly?\nTo do that, we can use the pytensorverify_gradutility:\n\nIt didn't raise anError, so we're clear!\nNow we can use our wrapped Op directly with PyMC models:\n\nPyMC providesmodel_to_graphvizto visualize the model graph:\n\nPart of verifying that theOp's API works with PyMC\nis in evaluating the model'slogpanddlogp.\nFirst, thelogpfunction:\n\nAnd now thedlogpfunction:\n\nThe numerical values of the evaluatedlogpanddlogpare correct in both cases.\n\nIf we want to use PyTensor's JAX backend,\nwe have to, somewhat paradoxically, tell PyTensor how to convert our Ops to JAX code.\nPyTensor does not know it was JAX code to begin with!\nFortunately, this is pretty simple by simply returning the original functions.\n\nNote that we don't return the jitted functions,\nbecause we want PyTensor to use JAX to jit the whole JAX graph together.\n\nNow we can compile to the JAX backend and get the same results!\nFirst with thelogp:\n\nAnd now with thedlogp:\n\nWrapping a Diffrax ODE Solver\n\nLet's move on to a more complicated situation.\nWe will wrap an ODE solver from the Diffrax library in this second example.\nIt will be a straightforward example with a single variable parameter:\nThe initial pointy0.\nFirst, we importdiffrax:\n\nThen we set up a simple ODE.\n\nFor those who are not familiar with ODEs,\nthevector_fieldis the derivative of the functionywith respect tot,\nand thetermis the ODE itself.\nThesolveris the method used to solve the ODE;\nthesaveatis the collection of times at which we want to save the solution;\nand thestepsize_controlleris used to control the step size of the solver.\nFinally,solis the solution to the ODE, evaluated at thesaveatpoints.\n\nFrom this point onward,\nthe rest of the code should look very similar to what we did above.\n\nFirstly, we need a JAX function that we will wrap.\nOur function will return the solutions forys, given a starting pointy0.\nThe other parameters will be constant for this example,\nbut they could also be variables in a more complexOp.\n\nThen, we define the vjp function.\n\nAfter that, we define theOpandVJPOpclasses for the ODE problem:\n\nAnd with no errors, we go on to register the JAX-ified versions of theOpandVJPOp:\n\nFinally, we can use theOpin a model,\nthis time to infer what the initial value of the ODE was from observed data:\n\nAs always, we can inspect the model's structure to make sure it is correct:\n\nAnd finally, we can verify that the model'slogpanddlogpfunctions execute.\nFirstly, without JAX mode:\n\nAnd then with JAX mode:\n\nAnd then thedlogpfunctions in both non-JAX and JAX mode:\n\nWrapping a Flax neural network\n\nOur final example will be encapsulating a Neural Network\nbuilt with the Flax library.\nIn this example, we will skip the gradient implementation.\nAs discussed below, you don't need to implement it\nif you defer the gradient transformation to JAX,\nas PyMC does when usingsampling.jax.\n\nIn this problem setup, we will be training a CNN\nto predict digit identity in a given MNIST dataset image.\nWe will make use oftensorflow_datasetsto get access to the MNIST dataset:\n\nWe can inspect the dataset to figure out its dimensions:\n\nHere, we selected 1,000 images, each of which is 28x28 pixels, with 1 channel\n\nLet's see what one of those images looks like:\n\nNow, we will implement a simple Convolution Neural Network (CNN)\nusing the very user-friendly Flax library.\n(It has an API that is very, very close in spirit to PyTorch.)\n\nThe exact structure of the model is unimportant here;\nwhatisimportant, though, is that the model is acallable.\n\nLet's initialize the CNN and iterate over the layers\nto get an idea of the number of parameters\n\nThis model has a lot of parameters,\nmany more than most of the classical statistical estimation models will have.\n\nWe can evaluate the forward pass of the network by callingcnn.apply.This is the function we want to wrap for use in PyMC.\n\nWe want to pass the weights of each kernel as vanilla arrays,\nbut FLAX requires them to be in a tree structure for evaluation.\nThis requires using some utilities, but it's otherwise straightforward.\nNote that this is specific to Flax, of course.\n\nIf you are feeling a bit confused\nbecause of the presence of \"unflattened\" and \"flattened\" parameters,\ndon't worry: it's just a technicality we need to deal with now.\nWhat's worth noting here is that the CNN's forward pass\nis wrapped in a JAX function that will be wrapped in a PyTensor Op,just as we had done before.\n\nNow, let's create the CNN Op.\nNote that we don't implement the gradient method!\n\nWe can now create a Bayesian Neural Network model,\ngiving a Normal prior for all the parameters in the CNN.\n\nAs before, we can compute the logp at the models' initial point,\nwhich lets us figure out whether there are any issues with the model or not.\n\nWe can do the same with the JAX backend.\n\nAs we mentioned, we don't always need to define the gradient method.\nFor instance, when using JAX samplers such assample_numpyro_nuts,\nthe gradients will be directly obtained from the jax compiled function.\n\nLet's confirm this is the case,\nby using the PyMC helperget_jaxified_logpthat returns the JAX function that computes the model joint logp,\nand then taking the gradient with respect to the first set of parameters.\nFirstly, we use theget_jaxified_logphelper to get the JAX function\n(and we evaluate it below):\n\nAnd then, we take the gradient with respect to the first set of parameters\nand evaluate it below as well:\n\nSummary\n\nWe hope you found this introduction to using PyMC with JAX helpful.\nJAX is a powerful automatic differentiation library,\nand a growing ecosystem is forming around it.\nPyTensor is a flexible library for the compilation and manipulation of symbolic expressions,\nfor which JAX is one supported backend.\nWe hope that this introduction will help you to use JAX with PyMC,\nand that you will find it helpful in your work!",
    "word_count": 1410
  },
  {
    "title": "My Journey Building PyMC Labs: Five Principles from Open Source that Boost Innovation at any Company",
    "url": "https://www.pymc-labs.com/blog-posts/labs-principles",
    "date": null,
    "author": null,
    "description": "5 organizational principles that foster innovation at PyMC Labs",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Likelihood Approximations with Neural Networks in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/likelihood-approximations-through-neural-networks",
    "date": null,
    "author": null,
    "description": " We use an example from cognitive modeling to show how differentiable likelihoods learned from simulators can be used with PyMC.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Marketing Mix Modeling : A Complete guide",
    "url": "https://www.pymc-labs.com/blog-posts/marketing-mix-modeling-a-complete-guide",
    "date": null,
    "author": null,
    "description": "This blog explains how Marketing Mix Modeling separates base and incremental sales, adds lag and saturation effects, and uses Bayesian methods to handle uncertainty. It also shows how these ideas are applied in practice, from early PyMC models at HelloFresh to more recent work with PyMC-Marketing at companies like Bolt.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Estimating a Candidate's Popularity over Time with Markov Processes",
    "url": "https://www.pymc-labs.com/blog-posts/markov-process",
    "date": null,
    "author": null,
    "description": " Estimate French presidents' popularity across time with a Markov model",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Unobserved Confounders, ROAS and Lift Tests in Media Mix Models",
    "url": "https://www.pymc-labs.com/blog-posts/mmm_roas_lift",
    "date": null,
    "author": null,
    "description": "Understanding the role of lift tests in calibrating Media Mix Models.When working with Media Mix Models (MMMs), calibration with lift tests is not just a technical step \u2014it's essential for making these models reliable and actionable.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Bayesian Media Mix Models: Modelling changes in marketing effectiveness over time",
    "url": "https://www.pymc-labs.com/blog-posts/modelling-changes-marketing-effectiveness-over-time",
    "date": null,
    "author": null,
    "description": " we outlined what Bayesian Media Mix Models (MMM's) are, how they worked, and what insights they can provide.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Out of model predictions with PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/out-of-model-predictions-with-pymc",
    "date": null,
    "author": null,
    "description": null,
    "content": "",
    "word_count": 0
  },
  {
    "title": "Customer Lifetime Value in the non-contractual continuous case: The Bayesian Pareto NBD Model",
    "url": "https://www.pymc-labs.com/blog-posts/pareto-nbd",
    "date": null,
    "author": null,
    "description": "Exploring the Bayesian Pareto NBD model for predicting customer lifetime value and purchase behavior",
    "content": "",
    "word_count": 0
  },
  {
    "title": "LLMs and Price Reasoning: Toward an Industry Benchmark",
    "url": "https://www.pymc-labs.com/blog-posts/price-benchmark",
    "date": null,
    "author": null,
    "description": "Novel LLM benchmark for evaluating the ability to estimate consumer product prices and reason strategically in an analogue of \"The Price is Right\" showcase game.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Probabilistic Time Series Analysis: Opportunities and Applications",
    "url": "https://www.pymc-labs.com/blog-posts/probabilistic-forecasting",
    "date": null,
    "author": null,
    "description": "Profound impact of Bayesian modelling on businesses decisions.",
    "content": "Probabilistic Time Series Analysis: Opportunities and Applications\n\nSeptember 23, 2025\n\nByJuan Orduz\n\nOver the past few years, we've witnessed the profound impact of Bayesian modeling on businesses' decisions.\nIn this post, we aim to delve into a specific area where Bayesian methods have demonstrated their transformative\npotential: probabilistic forecasting models.\n\nForecasting is a critical component of business planning across industries\u2014from retail inventory management to\nfinancial market analysis, from energy demand prediction to marketing budget allocation. Traditionally, businesses\nhave relied on point forecasts that provide a single estimate of future values. While these approaches can work\nreasonably well under stable conditions, they often fail to capture the inherent uncertainty in real-world systems\nand can lead to suboptimal decisions when confronted with volatile or complex environments.\n\nProbabilistic forecasting addresses these limitations by generating complete probability distributions over possible\nfuture outcomes rather than single-point predictions. This paradigm shift provides decision-makers with a more\ncomprehensive view of potential scenarios, enabling robust planning that accounts for risk and uncertainty. With\nrecent advances in computational methods and Bayesian statistics, these sophisticated approaches have become\nincreasingly accessible to practitioners.\n\nIn this post, we'll explore how probabilistic forecasting models can provide a competitive advantage through their\nability to incorporate domain expertise, handle data limitations, and model complex relationships. We'll demonstrate\nthese capabilities through several case studies that showcase practical applications across different business contexts.\n\nClassical Time Series Forecasting Models\n\nIn many business domains, such as logistics, retail, and marketing, we are interested in predicting the future values\nof one or more time series. Typical examples are KPIs like sales, conversions, orders, and retention. When businesses\nencounter forecasting challenges, they typically have two main approaches to consider:\n\nUse statistical methods to infer the trend, seasonality, and remainder components. Once we understand these components,\nwe can use them to predict the future. Classical examples that have been widely applied are exponential smoothing and\nautoregressive models. These models are great baselines and relatively easy to fit. Use external regressors and lagged\ncopies of the time series to fit a machine-learning model, such as linear regression or a gradient-boosted tree model.\n\nDepending on the data and concrete applications, either of these methods can perform very well. In fact, with modern\nopen-source packages like the ones developed byNixtla, it is straightforward to test and\nexperiment with these models (seestatsforecastfor statistical models\nandmlforecastfor machine learning forecasting models)).\n\nOf course, forecasting is a well-studied domain (see, for example,Forecasting: Principles and Practicefor an introduction), and there are many more approaches\nand combinations of these methods. For instance,\nthe articleM4 Forecasting Competition: Introducing a New Hybrid ES-RNN Modeldescribes a hybrid method between exponential smoothing and recurrent neural networks that outperformed many classical\ntime series models in theM4 competition. Trying to summarize\nall the models and possibilities in a blog post is nearly impossible. Instead, we want to describe some concrete\ncases where Bayesian forecasting models can be a competitive advantage for business applications.\n\nProbabilistic Forecasting Models\n\nGenerally speaking, when discussing probabilistic forecasting models, we often refer to a forecasting model learning\nparameters of a distribution instead of just the point forecast. For example, we can predict the mean and the standard\ndeviation when using a normal likelihood. This strategy provides great flexibility, as we can better control the\nuncertainty and model the mean and variance jointly. A well-known example of these models is theDeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networksdeveloped\nby Amazon Research.\n\nIn addition, we can go fully Bayesian by setting priors on the model parameters. The blog postNotes on Exponential Smoothing with NumPyroprovides an explicit description of how to implement anexponential smoothing modelas a fully Bayesian probabilistic forecasting model.\n\nOkay, and so what? What benefits can we get from these types of models? We are glad you asked \ud83d\ude09! Next, we present various\ncase studies where probabilistic forecasting models can significantly improve performance.\n\nHierarchical Models\n\nOne typical scenario where Bayesian models shine is when we have a hierarchical structure (e.g., category levels, region groupings).\nIn this case, sharing information across related groups can improve model performance by regularizing parameter\nestimation (the shrinkage effect). For a detailed description of this approach, please see the complete write-upHierarchical ModelingbyMichael Betancourt. In the context of time series forecasting, we can better\nestimate trend and seasonality components by leveraging the information-sharing nature of hierarchical models.\n\nLet's look at a concrete example to illustrate this point. Consider thequarterly tourism In Australia dataset,\nwhere we have tourism volumes between 1998 and 2016 per state, region, and purpose. For illustration, let us consider\nforecasting these tourism volumes for each state, region, and purpose combination (308 time series). We can take a look\nat a sample from the state of \"Victoria\" in the following plot:\n\nWhile subtle, we see a positive trend component in most of these sample series.\n\nA natural approach would be to consider each series independently and use a time series model, such as exponential\nsmoothing. This approach is a great baseline, but we can do better! In this example, there is a shared trend component\nacross regions, which might not be evident in the historical data of some smaller areas. Usinghierarchicalexponential\nsmoothing, we can improve the model performance, particularly for these smaller areas, as illustrated in the following plot:\n\nWe clearly see that in the test period, a mild trend component is inferred from the hierarchical structure\n(orange and green lines), which would likely not have been captured by a purely univariate time series model (pink line).\n\nThe effort to adapt an exponential smoothing model to a hierarchical version is not as complicated as it might sound.Hereyou can find all the code and details to\nreproduce this result (and potentially adapt it for your application \ud83d\ude09).\n\nRemark [Forecasting Baselines]:In this simple case, the hierarchical model performs slightly better than independentAutoETSmodels fitted to each individual series.\nThis result reflects the general situation where strong baseline models are often hard to beat. So please establish baseline\nmodels and robust evaluation processes before implementing more complex models \ud83d\udca1!\n\nCensored Likelihoods\n\nIn demand forecasting, we often face the challenge of censored data, where observations are only partially known.\nCensoring occurs when values above (or below) a certain threshold are unobserved or replaced with the\nthreshold value itself. This is particularly relevant in retail, where sales data only captures observed demand when products\nare in stock. The true, unconstrained demand remains unobserved during stockouts or when inventory capacity limits are reached.\n\nThe simulation studyDemand Forecasting with Censored Likelihooddemonstrates how\nBayesian models with censored likelihoods can provide more accurate demand forecasts compared to traditional time series\nmethods such as ARIMA. By simulating true demand as an AR(2) process with seasonal components and then generating censored\nsales data reflecting stockouts and capacity constraints, the study shows how traditional forecasting methods systematically\nunderestimate future demand because they treat the observed sales as the complete signal.\n\nLet's examine the simulated data:\n\nThe top plot shows the true underlying demand (black) and the sales that would have occurred without availability constraints (blue).\n\nThe bottom plot shows the expected sales without constraints (blue) versus the observed sales (orange) when availability\nconstraints impose a maximum sales value (note: this upper bound often varies over time).\n\nUltimately, we want to forecast the unconstrained demand (black curve) while only observing the constrained sales (orange curve). The\nfollowing plot compares the forecasts from a simple AR(2) seasonal model versus the censored likelihood model:\n\nThe results highlight a significant advantage of censored likelihood models: they accurately model the underlying demand\ndistribution even during periods with stockouts or capacity constraints, effectively \"reconstructing\" what the true demand\nwould have been. This is clearly visible as the predictions (pink) from the censored model are much closer to the true demand target\n(black) than those from the simple AR(2) model (green).\n\nThis modeling approach leads to forecasts that better capture both the\ntrue magnitude and the uncertainty of future demand. It provides critical information for inventory planning, capacity decisions,\nand revenue optimization that would be missed by conventional forecasting techniques ignoring the censoring mechanism.\n\nAvailability-Constrained TSB Models for Intermittent Series\n\nIn retail, intermittent time series are the norm rather than the exception. While top-selling products might show regular\ndaily sales patterns,the vast majority of items in a typical retail catalog exhibit sporadic demand- with many days\nshowing zero sales followed by occasional purchases. This pattern is particularly common for niche products, seasonal\nitems, or products with long replacement cycles. Traditional forecasting methods, often designed for continuous demand streams, struggle with these patterns.\n\nFor products with sporadic demand patterns (intermittent time series), the Teunter-Syntetos-Babai (TSB) model is a\npopular forecasting approach. However, standard TSB models cannot distinguish between true zero demand (no customer interest) and zeros caused\nby product unavailability (out of stock). The case studyHacking the TSB Model for Intermediate Time Series to Accommodate for Availability Constraintsdemonstrates how to extend the TSB model to account for availability constraints in intermittent demand forecasting.\n\nThe study, motivated byIvan Svetunkov's\"Why zeroes happen\"blog post,\nsimulates intermittent time series using a Poisson process combined with a binary availability mask. By incorporating this\navailability information directly into the model, it prevents the estimated probability of non-zero demand from dropping excessively when zeros are observed due to unavailability rather\nthan a lack of demand. The modified TSB model better preserves the true demand signal when forecasting, as it can\ndistinguish between zero sales caused by unavailability versus actual zero demand. Let's look at a specific time series prediction:\n\nDespite the fact that the last two data points of the training data are zeros, the model does not simply predict zeros for\nthe next time steps, which is a common behavior for standard intermittent time series models. The reason is that the model is aware\nthat these last two zeros are due to the lack of availability and not actual zero demand. Hence, when we set the\navailability to one for the future forecast period, the forecast is appropriately non-zero. This is exactly what we want \ud83d\ude4c!\n\nThis approach demonstrates the flexibility of probabilistic models to incorporate business constraints directly into the\nforecasting process. The results show significant improvement in forecast accuracy, particularly for products with\nlimited availability histories, providing more reliable demand estimates for inventory planning and allocation. The\nstudy also highlights how these models can be efficiently implemented and scaled to handle thousands of time series\nsimultaneously, making them practical for real-world business applications.\n\nCalibration and Custom Likelihoods\n\nProbabilistic forecasting models can be further enhanced through parameter calibration using domain knowledge or\nexperimental data. The case studyElectricity Demand Forecast with Prior Calibrationdemonstrates how to incorporate external information to improve a dynamic time-series model for electricity demand\nforecasting. The core idea is to model electricity demand as a function of temperature:\n\nWhen examining the ratio of electricity demand to temperature, we observe that this ratio is not constant but\ndepends on the temperature itself. Hence, we can use a Gaussian process to model this relationship.\nThe specific model uses a Hilbert Space Gaussian Process (HSGP) to capture the potentially complex, time-varying relationship between temperature\nand electricity demand.\n\nA key innovation here is the use of a prior calibration process to constrain the estimated temperature effect on demand, particularly\nfor extreme temperatures where historical data might be limited. By incorporating domain expertise on how electricity demand\nresponds to very high temperatures (e.g., above 32\u00b0C due to air conditioning load), the model produces more reliable forecasts in these edge cases. The\ncalibration works by adding custom likelihoods that effectively \"inform\" the model about expected parameter values in certain\nregimes. In this concrete example, we use a Gaussian process to model the temperature-electricity relationship while\nimposing a constraint on this ratio for high temperatures:\n\nThis estimated effect plot aligns with the observations made in the exploratory data analysis section ofForecasting: Principles and Practice (2nd ed.)by Hyndman and Athanasopoulos:\n\nIt is clear that high demand occurs when temperatures are high due to the effect of air-conditioning. But there is also a heating effect, where demand increases for very low temperatures.\n\nIndeed, our model captures this: at the extremes of the common temperature range, the temperature effect on demand increases.\nHeating and cooling demand typically increases outside the approximate range of 15\u00b0C - 25\u00b0C.\n\nThis calibration approach mirrors the methodology used inPyMC Marketing'sMedia Mix Model with Lift Test Calibration,\nwhere experimental results (lift tests) are used to calibrate marketing effectiveness parameters. In both cases, we enhance the\nforecasting model by incorporating additional likelihood terms that reflect external knowledge\u2014whether that's experimental\nlift test results, known physical constraints, or other domain expertise. This calibration technique is especially valuable when\nhistorical data doesn't fully represent all possible future scenarios or when we suspect the presence of unobserved confounders\n(seeUnobserved Confounders, ROAS and Lift Tests in Media Mix Models).\n\nIt is important to emphasize that these ideas are not new. The simulation example above is inspired by a similar technique\nfrom thePyrotutorialTrain a DLM with coefficients priors at various time points.\nSimilarly, extensions of this approach exist for larger time-series architectures likeCausal DeepARdeveloped byZalando Research.\n\nRemark [Scaling Probabilistic Forecasting Models]:Depending on the size of your dataset, you might consider\nrunning full Markov Chain Monte Carlo (MCMC) or opting for faster approximate inference methods like Stochastic Variational Inference (SVI), as described in NumPyro's tutorialHierarchical Forecasting.\n\nHierarchical Price Elasticity Models\n\nProbabilistic modeling isn't limited to time series applications; it can also provide powerful solutions for price\nsensitivity analysis. The case studyHierarchical Pricing Elasticity Modelsdemonstrates how Bayesian hierarchical models can significantly improve price elasticity estimates in retail settings,\nespecially where data may be sparse at the individual product level.\n\nUsing a retail dataset with over 5,000 SKUs across nearly 200 categories, the study compares three approaches to\nmodeling the constant elasticity demand function:\n\nA simple model where each SKU has independent elasticity estimates.\n\nA fixed-effects model accounting for date effects.\n\nA hierarchical model that incorporates the product category structure.\n\nThe hierarchical approach shows distinct advantages by leveraging the multilevel structure inherent in retail data. When\nindividual SKUs have limited price variation or sparse sales data, the hierarchical model \"borrows strength\" from\nsimilar products within the same category, producing more stable and economically sensible elasticity estimates.\n\nA key insight from this study is how the hierarchical model regularizes extreme elasticity values often caused by data\nsparsity or outliers. Where simple, independent models might estimate implausible positive price elasticities for some\nproducts due to noise, the hierarchical structure pulls these estimates toward more reasonable category-level means\nthrough partial pooling. This shrinkage effect is especially valuable for retailers with large product catalogs where\nmany items inevitably have limited historical price and sales data.\n\nThe approach also enables elasticity estimation at different levels of the product hierarchy (e.g., individual SKU, category,\ndepartment), allowing analysts to understand price sensitivity at multiple relevant decision-making levels. The implementation\nusing stochastic variational inference in NumPyro demonstrates that these sophisticated models can scale efficiently\nto large retail datasets with thousands of products, making them practical for real-world pricing applications.\n\nState Space Models in PyMC\n\nFor time series forecasting, state space models (SSMs) provide a powerful and flexible framework capable of handling complex\ndynamics, including time-varying trends, multiple seasonality patterns, and the influence of external regressors. The PyMC ecosystem recently expanded with the\nintroduction ofPyMC-Extras, which includes a specialized\nmodule for structural time series modeling based on state space representations\n(primarily developed byJesse Grabowski).\n\nTheStructural Time Series Modeling notebookdemonstrates how to leverage these components to build sophisticated forecasting models within PyMC. The module provides building\nblocks that can be combined to create custom models tailored to specific needs, including:\n\nLocal Linear Trend: Captures time-varying level and slope components.\n\nSeasonal components: Models periodic patterns with flexible frequencies (e.g., daily, weekly, yearly).\n\nRegression components: Incorporates the static effects of covariates.\n\nDynamic regression coefficients: Allows for time-varying relationships between predictors and the target variable.\n\nCycle components: Models quasi-cyclical behaviors with stochastic periodicity and damping factors.\n\nWhat makes this implementation particularly powerful is its composability \u2013 you can easily mix and match these components\nto construct models that reflect the specific characteristics and expected dynamics of your data.\n\nThe implementation relies on an efficient Kalman filter algorithm for state estimation and likelihood calculation. This allows for seamless handling of missing data,\nrobust anomaly detection, and integrated forecasting all within a unified Bayesian framework. This approach also facilitates the incorporation of domain knowledge\nthrough informative priors on the variance parameters, which control the flexibility and rate of change for each model component over time.\n\nA key advantage of the state space approach is its interpretability. It decomposes the time series into distinct, understandable components,\nmaking it easier to communicate insights to stakeholders. For example, you can extract and visualize the estimated trend,\nseasonal effects, and regression impacts separately, providing clear explanations of the factors driving your forecasts.\n\nFor practitioners looking to move beyond traditional or \"black-box\" forecasting methods, PyMC-Extras' state space module offers a\nfully Bayesian approach that explicitly models uncertainty while maintaining high interpretability. This is especially valuable\nin business contexts where understanding thewhybehind predictions is often as important as the predictions themselves.\n\nIf you want to learn more about this module, check out our webinar:\n\nSummary\n\nIn this blog post, we've explored several ways probabilistic forecasting models can provide significant advantages for\nbusiness applications compared to traditional time series methods. We've seen how hierarchical models can leverage shared\ninformation across related time series to improve forecasts, especially when dealing with sparse data. Censored likelihoods help\novercome limitations imposed by historical data, enabling us to model the true underlying demand even when observations are truncated\nby stockouts or capacity constraints. For intermittent demand patterns, availability-constrained TSB models distinguish\nbetween true zero demand and stockout-induced zeros, leading to more accurate forecasts for sporadically selling products.\n\nWe've also demonstrated how domain knowledge can be incorporated through model calibration using custom likelihoods. This\ntechnique proves particularly valuable when historical data contains unobserved confounders or doesn't fully capture all relevant dynamics, allowing us to understand not\nonly the forecast but also thedriversbehind it. Finally, we've highlighted the powerful state space modeling capabilities now available in\nPyMC-Extras, which provide flexible, interpretable components for building sophisticated Bayesian time series forecasting models.\n\nAcross all these applications, the common thread is clear: probabilistic forecasting doesn't just quantify uncertainty\nmore effectively\u2014it enables us to directly incorporate business constraints, domain knowledge, and potentially causal relationships into our\nmodels. This leads to forecasts that are not only potentially more accurate but also more actionable and interpretable for decision-\nmakers. As computational tools and methods continue to evolve, these sophisticated approaches are becoming increasingly accessible to practitioners,\nmaking probabilistic forecasting a valuable addition to any modern data scientist's toolkit.\n\nAdditionally, we've shown how these probabilistic approaches extend beyond traditional time series forecasting into areas like\nprice elasticity modeling. Here, hierarchical Bayesian methods can regularize elasticity estimates for large product\ncatalogs by leveraging the natural hierarchy in retail data. This ability to produce reliable estimates at different\nlevels of aggregation (from individual SKUs to product categories) while effectively handling data sparsity demonstrates the\nversatility of probabilistic modeling for critical business decision-making beyond pure forecasting tasks.",
    "word_count": 3179
  },
  {
    "title": "Running PyMC in the Browser with PyScript",
    "url": "https://www.pymc-labs.com/blog-posts/pymc-in-browser",
    "date": null,
    "author": null,
    "description": "Experience the power of PyMC in your browser with PyScript",
    "content": "Running PyMC in the Browser with PyScript\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nHow is that possible? PyScript!\n\nPyScriptis a new (as of 2022) tool that allows you to execute Python directly in your browser. Previously, running code client-side was only possible with JavaScript.\n\nWhat's really exciting is that it's not just a small subset of Python, but everything. You can even import packages likeNumPy,Pandas, andMatplotlib. The way this works is viaPyodide, a port of the CPython runtime implemented in WebAssembly.\n\nIf you want to learn more, watchPeter Wang'sPyCon 2022 Keynotewith many demos.\n\nPossible to run PyMC?\n\nNaturally, I was curious if it was possible to run PyMC through PyScript. On first thought this might seem impossible because PyMC compiles the model evaluation code to C or JAX (throughPyTensor). However,PyTensoralso has a Python mode which, while being much slower, is fully functional.\n\nWhy?\n\nBefore, you could have a PyMC model run on the server and then send the results back to the client (i.e. the browser). However, this has a few short-comings:\n\nIt's challenging to set everything up to handle the interplay between client and server correctly, with many different technologies interacting in complex ways\n\nOn top of the server<->client interplay, you have to pay special attention to scaling as many users might be running compute-extensive PyMC models in parallel\n\nUsers might not be comfortable to send their data to your server\n\nIf we can just run PyMC in the browser directly, all these problems go away. There is no interplay between client and server because everything runs on the client. There are no scaling issues because users use their own CPUs to fit their models. And finally, no data ever gets transmitted to the server, so it's completely safe and privacy preserving.\n\nThe Process\n\n1. Getting PyMC installed in the browser\n\nInPyScriptit's possible to install any packages that are onPyPIusingmicropip.\n\nThis installsbokehandpymcin your browser and we canimport pymc as pm. Easy-peasy.\n\n2. Write model\n\nNext, we can embed our Python code inpy-scripttags:\n\nNote that becausearviz(whatPyMCuses for plotting), has support forbokeh, a Python-to-JS plotting library, we can also get interactive plots.\n\n3. That's it!\n\nThere is no 3, you just open the website in your browser, it will install the packages and that's it!\n\nI was surprised by how simple it was to get this going, it took me a couple of hours to put everything together. These are really interesting times we're living in.\n\nApplications\n\nSo what could we do with this? Well, the possibilities are endless. The main applications will resolve around two possibilities:\n\nSampling a PyMC model based on user-data\n\nHaving a presampled model that we use to make predictions based on user-data\n\nSome example applications could be:\n\nJupyterLiteis a Jupyter NB that runs completely in the browser. So you can interactively work with PyMC without having to install anything. This would makeour corporate workshopseven simpler. Update:This already works.\n\nOne of our clients at PyMC Labs perform adaptive psychometric testing. The most informative questions are chosen using a Bayesian model. Currently, there are two versions of the model, one in PyMC that is fit infrequently on a batch of data and one in JavaScript for running in the browser while the subject is doing the test. In the future, they can just use the same PyMC model and don't have to have two separate versions.\n\nBambiallows to build generalized linear models with a single line that specifies the model. This could easily be turned into a webapp that allows users to upload their data and fit hierachical linear models to it.\n\nAlex Andorraruns awebsite for electoral forecastingusing PyMC. Seeherefor details on the model. Currently it just displays the latest plots generated on the backend, but with this it would allow for custom, user-defined forecasts.\n\nWith HelloFresh we have built astate-of-the-art Bayesian marketing mix model. They currently have to run this in Jupyter NB locally and send the results to stake-holders by email. This would make developing a webapp to interact with the data much simpler.\n\nSummary\n\nThis feels like a new dawn. JavaScript is by far the most commonly used programming language on the planet. Not because it's a great language (it's OK) but because you can execute it on everything that can run a browser.\n\nThis universality is now coming to Python, giving web programmers access to its rich ecosystem, including the PyData stack. And with this blog post, you can also run complex Bayesian models in PyMC.\n\nI cannot wait to see what amazing things the community will produce around this!\n\nResources\n\nCode\n\nPeter Wang's PyCon 2022 Keynote\n\nPost by Simon Willison that was instrumental in getting this to work\n\nRavin Kumar's explorations of PyScript and getting PyMC to run",
    "word_count": 784
  },
  {
    "title": "PyMC-Marketing: A Bayesian Approach to Marketing Data Science",
    "url": "https://www.pymc-labs.com/blog-posts/pymc-marketing-a-bayesian-approach-to-marketing-data-science",
    "date": null,
    "author": null,
    "description": "PyMC Labs is excited to announce the initial release of PyMC-Marketing",
    "content": "",
    "word_count": 0
  },
  {
    "title": "PyMC-Marketing vs. Meridian: A Quantitative Comparison of Open Source MMM Libraries",
    "url": "https://www.pymc-labs.com/blog-posts/pymc-marketing-vs-google-meridian",
    "date": null,
    "author": null,
    "description": "This benchmark study directly compares PyMC-Marketing and Google\u2019s Meridian on realistic synthetic datasets, from startup-scale to enterprise-level. Using aligned priors and identical configurations, it shows that PyMC-Marketing is consistently faster (2\u201320x), more accurate (lower error in channel contribution recovery, higher R\u00b2, lower MAPE), and more scalable, thanks to its flexible sampling backends. Meridian remains leaner in storage size but suffers from slower performance, wider uncertainty, and convergence issues at scale. Overall, PyMC-Marketing emerges as the more robust, production-ready MMM library for data science teams.",
    "content": "",
    "word_count": 0
  },
  {
    "title": "MCMC for big datasets: faster sampling with JAX and the GPU",
    "url": "https://www.pymc-labs.com/blog-posts/pymc-stan-benchmark",
    "date": null,
    "author": null,
    "description": "Scaling PyMC using JAX to sample on the GPU",
    "content": "",
    "word_count": 0
  },
  {
    "title": "Improving the Speed and Accuracy of Bayesian Media Mix Models",
    "url": "https://www.pymc-labs.com/blog-posts/reducing-customer-acquisition-costs-how-we-helped-optimizing-hellofreshs-marketing-budget",
    "date": null,
    "author": null,
    "description": "Using Bayesian MMM's to reduce customer acquisition costs",
    "content": "Improving the Speed and Accuracy of Bayesian Media Mix Models\n\nSeptember 23, 2025\n\nByBenjamin Vincent\n\nThe data scientists atHelloFreshhave a big job on their hands. As part of a rapidly growing company with a worldwide reach, they influence the allocation of marketing dollars every year. One of their tasks is to maximize new customer acquisitions from data-driven insights.\n\nAs part of their approach, the data scientists built a BayesianMedia Mix Model(seethis videofor more details). If you want a refresher of the power of Bayesian Media Mix Models, check out ourprevious blog post. But in short, MMM\u2019s help us understand the characteristics and effectiveness of different marketing channels like TV, radio, podcasts, social media, daily deals, and more, based on how much we spent on each channel over time and how many new users we acquired.\n\nGiven the scale of HelloFresh\u2019s operations, even minor improvements in the insights gained by such models can have significant effects on new customer acquisitions. Recently HelloFresh\u2019s data scientists challenged us to do precisely this - could we (PyMC Labs) help them improve theiralready sophisticated Bayesian MMM?\n\nWhat we delivered to HelloFresh\n\nMore accurate and precise model predictions\n\nThanks toPyMC3, we can build a model, feed in data, and press the Inference Button\u2122. However, making high profile decisions requires due diligence! One crucial way of doing this is inspectingposterior predictive checks, which are plots comparing the model\u2019s predicted customer acquisitions to the actual data. The closer the match between predictions and actuals, the greater confidence we can have in the model. By inspecting PPC plots, we could identify two key improvements to the model:\n\nWe changed the outcome variable to be thelogarithmof the number of new customers rather than the actualrawnumber. This can be useful in situations where there might be signal-dependent noise (i.e. more measurement error at higher customer levels) and allow the model to predictproportionalchanges in the number of customers.\n\nIn addition, we reduced undesired effects on the parameter estimates by outlier data points by switching from a Normal likelihood function to aStudent Tdistribution which is more robust to outliers.\n\nWith these two changes, we improved both the accuracy and precision of the model\u2019s predictions. In the figure below the top panel shows the posterior of the original model and the bottom panel shows the posterior of the improved model. The comparison with actuals demonstrates reduced error and a 60% reduction in the variance of the predictions. The net result was that HelloFresh\u2019s data science team could gain greater confidence that parameter estimates (such as channel efficiency or saturation) capture something meaningful about the world, providing greater confidence in making data-driven changes to marketing budgets.\n\nIn addition, we implemented the following enhancements:\n\nWe changed the baseline static MMM by allowing parameters to evolve over time (we\u2019ll expand upon this in the next blog post in this series).\n\nWe changed how baseline customer levels and seasonal effects are added to the model.\n\nWe performed a sensitivity analysis on the influence of COVID-19 on model stability.\n\n10x speed up and additional insights\n\nAs the developers ofPyMC3, we could leverage our considerable familiarity and expertise to deliver a 10x speedup to model inference time. By reducing a 20 min process down to just 2 mins, rapid iteration and experimentation become possible.\n\nThis was achieved through multiple methods, but particular attention was paid to theadstock function. Initially, this was implemented using an expensive convolution operation that scaled quadratically with the number of data points. We derived a linear-time algorithm for this specific case and implemented a customtheanooperator withnumbaJIT compilation to achieve significant speedups.\n\nSpeeding up the model this much-allowedsensitivity analysesto be undertaken. We run hundreds of simulations under different priors to understand how it affected posterior estimates of the parameters. If parameter estimates are sensitive to priors\u2019 variation, we know they are influenced less by the data and more by beliefs, which is not appropriate if priors are not supported by solid domain knowledge.\n\nImproved interpretability of model parameters\n\nWe also reparameterized thereach functionas:\n\nWhereis the number of users at saturation, andis the initial cost per user (inverse of the slope at zero dollars). This helps in interpreting model parameters because they are clearly related to money spent. It also makes it easier to use human knowledge and experience to define priors over these parameters. The plot below shows an example of the new parametrization using some fake data.\n\nThe full package\n\nWe work closely with clients to make sure we help solve their problems. But the code is just one part of that. We provided:\n\nFully documented Jupyter notebooks that demonstrated the use of the new code.\n\nWeekly calls and project presentations to troubleshoot, keep goals aligned, and summarize our ongoing progress.\n\nFull training in the modeling and methods used so that the data science team was left empowered, able to experiment and debug independently from PyMC Labs long after the project was over.\n\nThis, and other projects, have also benefited from pre-release functionality of PyMC3. For example, this project used a newZeroSumNormaldistribution which was not publically available at the time.\n\nWe and the HelloFresh data science team (check out theirrecruitmentpage)  were delighted with the results of this project. But because Media Mix Modeling can be used in various ways, there are multiple routes for further progress. In fact, we are currently still working with the HelloFresh team on the topic. For example, Bayesian MMM\u2019s can be used to optimally and automatically set budgets across media channels, rather than justinformingthose budgets. And those budgets can be used not only to maximize customer acquisitions but also to drive further learning and marketing experimentation where we have remaining uncertainty.",
    "word_count": 940
  },
  {
    "title": "Introducing PyMC Labs",
    "url": "https://www.pymc-labs.com/blog-posts/saving-the-world",
    "date": null,
    "author": null,
    "description": "Discover how PyMC Labs is helping organizations solve complex problems using Bayesian modeling.",
    "content": "Introducing PyMC Labs\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nAfter I left Quantopian in 2020, something interesting happened: various companies contacted me inquiring about\nconsulting to help them with their PyMC3 models.\n\nUsually, I don't hear how people are usingPyMC3-- they mostly show up onGitHuborDiscoursewhen something isn't working\nright. So, hearing about all these really cool projects was quite exciting. However, I couldn't possibly take all of\nthese projects on by myself.\n\nThus, it was time to assemble a team of the most badass Bayesian modelers the world had ever seen -- the Bayesian\nAvengers, if you will. Fortunately, I did not have to venture far, as PyMC3 had already attracted exactly these types\nof people.\n\nThis brings me to the Big Announcement: For the last few months, we have quietly been buildingPyMC Labs, a Bayesian modeling consultancy.We have an amazing teamconsisting of three neuroscience PhDs, mathematicians,\nsocial scientists, a SpaceX rocket scientist, and the host of the famous\u2018Learning Bayesian Statistics\u2019 podcast. All of us are united in our mission:\n\nSaving the world with Bayesian modeling\n\nDoes this sound a bit grandiose? Probably. Is this true? I firmly believe it is. There are so many important problems\nthe world faces today -- from climate change to COVID19, from education to poverty -- and Bayesian modeling can play a\ncritical role in solving these problems. Let me explain why.\n\nIt is already doing it\n\nI would not have imagined it when I started contributing to PyMC, but the science PyMC3 has directly enabled ranges\nfromclimate scienceand biology to\nastronomy and zoology, and everything in between.\n\nFor instance, it was used to predict the spread of COVID19 in a recentScience paper,\nas well astrack the reproduction factor in real-time.\nIn both cases, the benefit of PyMC3 was its ease-of-use and the ability to integrate scientific domain knowledge and\nget honest uncertainty estimation in a highly volatile and uncertain situation.\n\nNow I know you\u2019re very observant and I hear you thinking: \u201cwait a minute, those benefits of Bayesian modeling sound\nquite general, so why would they be only valid for epidemiology?\u201d. And indeed they aren\u2019t! For similar benefits,\nPyMC3 is also used tofind planets outside of our solar systemandanalyze earthquakes. One of my coworkers here at PyMC Labs uses it forelectoral and political forecasting,\nbecause polls are noisy, scarce and need to be completed by domain knowledge -- one of the perfect settings for\nBayesian inference!\n\nWith all of this, at the time of writing, thePyMC3 paperhas been cited over 930\ntimes and is in the top 10 most cited articles of the entire PeerJ journal.\n\nSolving Business Problems\n\nBeyond scientific research, I find that PyMC3 is the perfect tool to also solve various business problems.\nAnd indeed it\u2019s already successfully used in production at companies as big and diverse as SpaceX, Roche,\nNetflix, Deliveroo and HelloFresh.\n\nThis diversity means that thePyMC Labs team intervenesto, for instance,build complex models from the latest finance research;\noptimize supply chains for food delivery; build software from top to bottom for pharmaceutical applications;\nspeed up and extend models for the farm tech industry; train and enhance any data science team\u2019s Bayesian stats\ncapacities, etc.\n\nPrediction Vs Inference\n\nAs data science has exploded in the last decade I have always been surprised by the over-emphasis on prediction-focused\nmachine learning. For far too long, it has been hailed as the solution to most of our data science problems.\n\nI believe that the potential of this is way overblown. Not because it doesn't work -- algorithms like deep nets or\nrandom forests are extremely powerful at extracting non-linear predictive patterns from large data sets -- but rather\nbecause most data science problems are not simplepredictionbut ratherinferenceproblems.\n\nIn addition, we often already have a lot of knowledge about our problem: knowledge of certain structure in our data\nset (like nested data, that some variables relate to some but not other parameters) and knowledge of which range of\nvalues we expect certain parameters of our model to fall into. Prediction-focused ML does not allow us to include any\nof this information, that's why it requires so much data.\n\nWith Bayesian statistics, we don't have to learn everything from data as we translate this knowledge into a custom model.\nThus, rather than changing our problem to fit the solution, as is common with ML, we can tailor the solution to best\nsolve the problem at hand. I like to compare this with Playmobil vs Lego:\n\nPlaymobil just gives you a single toy you can't change while Lego (i.e Bayes here) gives you building blocks to build\nthe toy you actually want. In Bayesian modeling, these building blocks are probability distributions.\n\nBut how do you do this in practice? This is where PyMC3 comes in, as it allows you to specify your models as Python\ncode and automatically estimate it without requiring manual mathematical derivations. Due to recent theoretical and\ntechnological advances, this also runs quickly and scales to complex models on large(ish) data sets.\n\nServing our mission\n\nSo how do we best make progress on our mission?\n\nFirst, we will continue to make PyMC3 the best, most user-friendly and scalable Bayesian modeling package out there.\nWe are well set up to do this, having a friendly API, a huge user-base, and a large developer team of over 20 active\nmembers. With our renewed focus onPyMC3 on Theano with a JAX backendall our resources will go towards this goal.\n\nSecond, our new PyMC consultancy will support this endeavour. It allows us to directly help clients use these powerful,\ncustomizable methods to solve their business problems, thereby increasing adoption and recognition.\nAs a great side effect, these client projects also help us find things that need to be fixed, improved or optimized\nin PyMC3, thereby lifting all (Bayesian) boats instead of just the happy fews\u2019.\n\nSo far, this has been an incredibly rewarding and exhilarating journey. Even though it is still early, we are learning\na lot about which areas Bayesian modeling is particularly well suited for but also what would make PyMC3 even better.\nWithout spoiling a future blog post that will go into more detail about what we have learned applying these methods,\nthe best use-cases include (but aren\u2019t limited to)incorporating domain knowledge, building bespoke models and\nquantifying uncertainty around estimates.\n\nSounds familiar? If you or your company has a problem for which prediction-based ML is not a good fit, I'd love to talk\nto you atinfo@pymc-labs.com. This is just the beginning and\nI hope you will join us on this marvelous journey.",
    "word_count": 1089
  },
  {
    "title": "Simulating data with PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/simulating-data-with-pymc",
    "date": null,
    "author": null,
    "description": "Explore how PyMC can be used for efficient and powerful data simulation.",
    "content": "Simulating data with PyMC\n\nSeptember 23, 2025\n\nByRicardo Vieira and Tom\u00e1s Capretto\n\nImage fromWikipedia\n\nPyMCprovids a great API for defining statistical models.\nWhen paired with its sampling algorithms, it becomes the ideal tool for conducting reliable and robust Bayesian inference.\n\nStill, Bayesian inference is far from its only use case.\nPyMC models specify a highly structured data-generating process that can be very useful on its own.\nApplications include simulation for optimization routines, risk analysis, and research design, among many others.\n\nPyMC comes with many user-friendly builtin distributions and meta-distributions\nwhich are cumbersome to write from scratch with NumPy or SciPy routines.\nThese include Mixtures, Timeseries, Multivariate parametrizations, Censored and Truncated distributions,\nand pretty much anything you would would ever need when doing data simulation.\n\nIn this blog post we will give you a flavor for some of these, and show how we use them as part of a data modelling workflow!\n\nTaking draws from simple distributions\n\nSciPy has a lot of distributions, but they are often difficult to work with, due to their focus on loc-scale parametrizations.\n\nPyMC tends to pick more intuitive parametrizations (and often offers multiple options).\n\nFor instance, in PyMC you can define aGammadistribution using\nthe shape/rate parametrization (which we call alpha and beta), and then take draws with thedrawfunction.\n\nOr, perhaps more intuitively, using the mean/standard deviation parametrization (called mu and sigma).\n\nPyMC takes care of converting between equivalent parametrizations for the user.\n\nAll PyMC distributions are vectorized\n\nNot all SciPy distributions allow NumPy-like broadcasting for their parameters.\nPyMC distributions always do!\n\nAh well...\n\nMeta-Distributions\n\nNeither NumPy nor SciPy offer a pre-built truncated LogNormal distribution (last time I checked).\nThey do offer a Truncated Normal, and you could exponentiate those to obtain Truncated LogNormal draws.\nBut what if you wanted to sample some other truncated distribution?\n\nPyMC can truncate any (pure univariate) distribution you throw at it, via theTruncatedclass.\n\nOr you can sample from Mixtures, usingMixture.\n\nHere we sample from a Mixture of two Normals, with weights[0.3, 0.7],\nmeaning that 30% of the draws will come from the first component and 70% from the second (on average).\n\nOr Random walks... with mixture initial distributions? Sure.\n\nMultiple variables\n\nYou can also draw multiple non-independent variables easily.\n\nIn this example we first sample a categorical index variable,\nwhich is then used to select an entry from a vector of 3 Normals with means [-100, 0, 100], respectively.\n\nWe can retrieve both the index and selected Normal draws viadraw.\n\nHere we first sample a Poisson variable.\nThis value then determines how many draws to take from a Gamma variable that is finally summed.\n\nWhat if I don't know what I want to sample?\n\nSometimes you have a rough idea of what the data looks like, but you don't know how to specify it.\n\nThis could happen in a parameter recovery study, where you want to simulate covariates that are somewhat \"realistic\".\nMaybe their marginals match an observed dataset or they show some level of covariance.\nBut you may not know exactly what parameters to use.\n\nThe good news is that PyMC can also do inference (that's its main goal after all)!\n\nLet's say we have the following marginal data,\nand we want to simulate something that behaves roughly like it.\n\nLooks like a positive distribution, maybe multimodal.\nPerhaps a LogNormal mixture?\n\nWhat about the parameters for the components, what should be the mixture weigths?\nLet's make PyMC infer them!\n\nLet's assume the price for each of the 5 cuts can be modeled by a mixture of 3 LogNormal distributions.\nThat will be 5 * 3 = 15 means and 15 standard deviations.\nWe will also need 15 mixture weights.\nSounds like many parameters, but we have much more data.\n\nLet's create a PyMC model,\nwith vague priors and fit the most likely parameter combination that could have generated the data.\n\nThat's a bit complicated... but not too bad once you write a couple of PyMC models.\nPyMC comes with a whole set of utilities to help you define complex statistical models.\n\nIn the example above we used coords,\nto specify the shape of our parameters with human-readable labels.\n\nWe can also request a graphical representation of our model:\n\nLooks about right.\nAnd here are the most likely parameters according tofind_MAP:\n\nNow that we have a set of parameter values,\nwe can take draws from the distribution of interest.\nHopefully it will resemble our data.\n\nThe marginal simulated histograms do resemble those from the original dataset.\n\nAnother way of checking their similarity is to look at the empirical CDF.\nThe two lines should look alike for distributions that are similar.\n\nLooks close enough!\n\nIf it didn't, we could go back to the Model and try something else.\nMaybe more components, or different distribution families...\n\nBut careful, if you do this enough times, you may end up as adata modellingpractitioner!\n\nTechnical advice\n\nIn the examples above we used the handydrawfunction.\nUnder the hood, this function creates a compiled function that takes draws from your specified variables, seeds it,\nand then calls it multiple times in a Python loop.\n\nIf you are writing performance-critical code,\nyou should avoid callingdrawin a loop,\nas it will recompile the same function every time it is called.\nInstead you can compile the underlying random function directly withcompile_pymcand reuse it whenever needed.\n\nThe internals ofdraware prettystraightforward.\n\nSecondly, if you need to take many draws from the same distribution,\nit's better to define it with the final shape and call the function only once.In the examples above we never did this!\n\nThis way, the random number generation can be vectorized.\n\nTo better understand PyMC shapes,\ncheck outthis page.\n\nConcluding remarks\n\nIn this blog post we showed how PyMC can be used as a powerful replacement for NumPy and SciPy\nwhen writing structured random generation routines.\n\nWe also hinted at how the same code can be reused for both simulation and inference in the last example.\nIf you go a bit further, you can start doing predictions based on your estimated statistical model.\nThis is, in a nutshell, what model based data science is all about!\n\nIf you are a data scientist, doing data simulation, we hope you give PyMC a try!",
    "word_count": 1033
  },
  {
    "title": "Modeling spatial data with Gaussian processes in PyMC",
    "url": "https://www.pymc-labs.com/blog-posts/spatial-gaussian-process-01",
    "date": null,
    "author": null,
    "description": "We build a Gaussian process model on a geospatial dataset with the goal of predicting expected concentrations of a radioactive gas in households depending on the county the houses belong to.",
    "content": "Modeling spatial data with Gaussian processes in PyMC\n\nSeptember 23, 2025\n\nByThomas Wiecki\n\nBeyond naive hierarchical models\n\nSo many times I've found myself having to work with geospatial data. It's everywhere: from advertisement, to product inventories, to electoral polls. While I was learning Bayesian modeling, I used to think about geographical information as some categorical value that grouped some observations together. The nice thing about this approach is that observations coming from the exact same geographical area will share a common feature, and this feature will be used (somehow) to explain the similarities between both observations. The bad thing about this approach is that observations from neighboring geographical areas are assumed to have absolutely nothing in common. Sounds weird, right? Usually we would picture some kind of continuous latent geographical feature that makes observations taken from nearby places be similar to each other. Surely there must be a better way of modeling geospatial data!\n\nCompletely independently from me modeling geospatial data, I learned how to work with Gaussian processes (GPs). GPs provide a very nice and flexible way of setting a prior that essentially says:\"nearby observations should be similar to each other, and as the observations go further away, they become uncorrelated\". This really clicked with what I wanted to do with geospatial data! The only problem was that there weren't many sources targeting general audiences that explained how to use GPs on geospatial data. That's why I wanted to put together a small example that showcases how you can use GPs with geospatial data using PyMC.\n\nOur dataset\n\nWe will be revisiting a classic: the radon dataset, fromGelman and Hill 2006(if you haven't read thePyMC case studyyet, you really should go ahead and do that now, it's an excellent resource!). Just to give you a quick refresher, Gelman and Hill studied a dataset of radon measurements that were performed in 919 households from 85 counties of the state of Minnesota. The case study cited above, focuses on how to use the county information to group households together, and then try to estimate a state-wide expected radon level, and the expected radon for each observed county. This is exactly the same as my old grouping approach to geospatial data!\n\nThe grouping approach comes with the crucial drawback I mentioned before: measurements from neighboring counties are uncorrelated from each other. This makes no sense! Why should the radon concentration in Earth's crust follow some county boundary line?! It makes much more sense to imagine that radon concentration varies continuously on the surface of the Earth, and then simply use the household locations to estimate it. This would actually give us the information of the radon we expect to measure in neighboring states or countries! Let's see how we can do this using PyMC!\n\nBut first, let's look at some maps\n\nWe can't start our modeling without first looking at our data on a map. So we'll do a small detour on how I usedcartopyto plot the radon dataset.\n\nCartopy can be used to get shape files from some public sources, likeNatural Earth. One of these are the shape files for counties in the United States. The county shapes have a lot of meta information. One important field is called FIPS, that stands forFederal Information Processing System. At the time the radon measurements were performed, the counties were identified using their FIPS codes, and we will use these to align our observations to the corresponding shape files.\n\nNow that we have loaded the dataset and extracted all the necessary shape files, we notice something interesting. The dataset has measurements for 85 out of 87 counties from Minnesota. We will use our model to predict the expected level of radon that we should see in the remaining 2 counties and also on the 37 neighboring counties from the neighboring states. Let's have a look at where these counties are on a map:\n\nThe green counties are the ones where we have at least one household measurement. The red ones are where we don't have measurements. The blue dots are points that are within the county. Since the radon dataset doesn't have the precise coordinates of each household (I imagine for privacy reasons), we will use the coordinates of the blue dots to impute the county households locations.\n\nNow let's look at the average radon measurement for each county. To view this, we first need to use a dictionary that maps from the county measured in the radon dataframe to the shapefile record for said county.\n\nWe can see a small pattern forming here. It looks like radon is denser to the south west, and sparser in the north east. We're almost ready to take this data to the next level!\n\nGaussian processes on a sphere\n\nEven though the above plot seems flat, Earth is round (kind of):\n\nAnd this makes it a bit harder for us to define Gaussian processes. Thankfully, there isa very nice review paperby Jeong, Jun and Genton that talks exactly about how one could write down covariance functions for Gaussian process that occur on the surface of a sphere. We will basically follow one of their simplest approaches: use a Mat\u00e9rn kernel that relies on the chordal distance between points.\n\nWait, thewhatdistance?!\n\nThe chordal distance simply is the distance between two points on the surface of the Earth if you could travel between them using a straight line that went through the planet. On the other hand, the Geodesic distance is the shortest distance between two points if one was forced to travel on the surface of the globe.\n\nCustom PyMC covariance kernel on a sphere\n\nPyMC comes with manycovariance kernels, but they all assume that the distance between two points is the Euclidean distance. Since we will be working with longitude and latitude pairs, we will need to write a customCovariancesubclass that operates using the metric we actually want - the chordal distance between points on a sphere.\n\nTo do this, we basically need to inherit frompymc.gp.cov.Stationaryand overload thefullmethod. This method computes the covariance between a set of points. To do this, we will copy the implementation frompymc.gp.cov.Matern32but change the distance function used. Instead of using the assumedeuclidean_dist, we will write a custom method, thechordal_dist. To compute the chordal distance, we only need to convert the longitude/latitude coordinates into their 3-D position counterparts. The chordal distance then simply is the Euclidean distance between the 3-D position coordinates.\n\nGaussian process geospatial model\n\nIt's been a long road up until now, but we are finally ready to write our PyMC model of the geospatial dataset! Let's go step by step, and start by simply creating the model instance and populating it with some data.\n\nNow, we will basically assume that the observed radon comes from a state level mean plus some deviation due to the county. The county deviation will be represented by a GP. Let's first create the state level random variables.\n\nWe create the GP prior for the county deviations. We will try to infer the length scale and a priori assume that it's around 200 km with a standard deviation of about 50 km.\n\nWe piece everything together and compute the expected radon measurement of each household:\n\nNow, the final piece of the puzzle: the observational distribution. We will model the log radon values\n\nIf you are familiar with the radon dataset you will have noted that we haven't included any information about whether the radon was recorded at the basement or not. We could exploit this extra information to make the model better. We decided to go with the simplest geospatial model posible to begin with, but if you want more, just ping us on Twitter@pymc_labs, and we'll write up a follow up blog post on how you could add this extra piece of information to the model.\n\nNow let's sample from the model and see what the model learns about the measured counties:\n\nFinally, the most important piece, extrapolating what we learned to unobserved counties! To do this, we will have to create conditional GP's on the unobserved coordinates, and then create the conditional observed log radon random variables:\n\nAfterwards, predicting on these new counties is as easy as sampling from the posterior predictive distribution.\n\nBeautiful! Our model learned the north-east south-west pattern of radon from the observations! I also takes advantage of the discovered pattern to make predictions of the expected radon in the neighboring counties, and in the two counties without measurements in Minnesota.\n\nAnd, of course, we can also visualize the uncertainty we have on our predicted expected log radon measurements by plotting the standard deviation of the posterior predictive distribution:\n\nThe model is quite certain in its prediction for the Minnesota counties, and the standard deviation of expected radon is small. But as the model is forced to make predictions outside of Minnesota, the standard deviation increases, meaning that the model is less certain about its mean predicted value. We can also see that the model is fairly certain about the predicted value for the two Minnesota counties without measurements, which makes sense, given that we have a bunch of recordings all around them.\n\nWrap up\n\nThat was quite a ride! We managed to build a Gaussian process prior for the radon dataset, which allowed us to learn a geospatial pattern of expected radon concentration, and to predict measurement concentrations in neighboring, unmeasured counties. To compare this against the grouping approach, which is sometimes called a hierarchical model, we are able to see a more informed extrapolation from observed counties to unobserved counties. The grouping approach, at its core, assumes that a priori all groups are exchangeable. In other words, a naive hierarchical model would only be able to predict that counties to the east and west of Minnesota would have exactly the same expected radon concentration.\n\nAs we mentioned earlier in this post, this model doesn't exploit part of the information present in the dataset. It can be improved to leverage whether a recording was taken in a basement or not, and it could also exploit the measured Uranium similarly to how it's shown in thegroup level predictors exampleusing the naive hierarchical model. If you're interested in reading a future blog post that shows this, feel free to reach out ontwitter.\n\nAcknowledgements\n\nCover photo byGreg RosenkeonUnsplash\n\nReferences\n\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511790942\n\nPyMC radon dataset Case study\n\nJaehong Jeong. Mikyoung Jun. Marc G. Genton. \"Spherical Process Models for Global Spatial Statistics.\" Statist. Sci. 32 (4) 501 - 513, November 2017.https://doi.org/10.1214/17-STS620\n\nRadon dataset from Gelman & Hill (2006). We only consider the recordings performed in Minnesota (MN) that go from row 5081 to 5999. A file with the relevant subset is available in thepymc-examples repository",
    "word_count": 1802
  },
  {
    "title": "Synthetic Consumers: The Promise, The Reality, and The Future",
    "url": "https://www.pymc-labs.com/blog-posts/synthetic-consumers",
    "date": null,
    "author": null,
    "description": "\ud83d\udce2 Announcing Our First White Paper: \"Synthetic Consumers: The Promise, The Reality, and The Future\"",
    "content": "Synthetic Consumers: The Promise, The Reality, and The Future\n\nSeptember 23, 2025\n\nByNina Rismal,Luca Fiaschi\n\nSynthetic consumers \u2013 AI-generated personas designed to simulate human consumer behavior \u2013 are rapidly transforming market research by deliveringfaster,cost-effective, and highly scalable insightscompared to traditional methods. By 2027,synthetic responses are expected to constitute over half of all marketresearch data,highlighting the urgency for businesses to understand and adopt this technology strategically.\n\nThis white paper equips technical business leaders, consumer insights experts, anddata scientists with clear and actionable knowledge about this technology.\n\nKey Insights\n\nDefining Synthetic Consumers: We clearly define synthetic consumers anddistinguish them from related concepts like digital twins, synthetic respondents,and human simulacra.\n\nDefining Synthetic Consumers: We clearly define synthetic consumers anddistinguish them from related concepts like digital twins, synthetic respondents,and human simulacra.\n\nReal-World Use Cases: We highlight practical examples of synthetic consumersapplications currently used by both major companies and startups for producttesting, innovation, data augmentation, and consumer insights.\n\nReal-World Use Cases: We highlight practical examples of synthetic consumersapplications currently used by both major companies and startups for producttesting, innovation, data augmentation, and consumer insights.\n\nAccuracy and Validation: We analyze methods for evaluating synthetic consumeraccuracy, summarizing recent research that increasingly shows confidence in thealignment between synthetic and human responses across multiple domains.\n\nAccuracy and Validation: We analyze methods for evaluating synthetic consumeraccuracy, summarizing recent research that increasingly shows confidence in thealignment between synthetic and human responses across multiple domains.\n\nPyMC Labs offers unique expertise by creating customized solutions that combineadvanced Generative AI with science-based benchmarking. Our rigorous andtransparent methods allow clients to confidently use synthetic consumer insights forclear business value.\n\nIn this paper, we aim to provide a balanced view on synthetic consumers,highlighting their transformative potential while recognizing their current limitations.\n\n###\ud83d\udcd6 Read the Full Paper Here\n\n\ud83d\udd17Download the white paper now\n\nJoin us in redefining market research. Reach out atinfo@pymc-labs.comorsubscribe to our newsletter:https://www.pymc-labs.com/newsletter/",
    "word_count": 308
  },
  {
    "title": "Can Synthetic Consumers Answer Open-Ended Questions?",
    "url": "https://www.pymc-labs.com/blog-posts/synthetic-consumers-open-ended-responses",
    "date": null,
    "author": null,
    "description": "Evaluating LLMs on generating open-ended text responses.Synthetic consumers are revolutionizing market research by making it faster, less expensive, and more flexible.",
    "content": "Can Synthetic Consumers Answer Open-Ended Questions?\n\nSeptember 23, 2025\n\nByAllen Downey\n\nEvaluating LLMs on generating open-ended text responses\n\nSynthetic consumers are revolutionizing market researchby making it faster, less expensive, and more flexible. One especially valuable capability is their ability to generate responses to open-ended questions. This enhances the depth and richness of insights typically associated with qualitative research.\n\nOf course, synthetic consumers are only useful if their responses are similar to those of real people. To see whether they are, we are conducting a series of experiments to test synthetic consumers using public datasets and representative survey questions.\n\nInthis previous post, we used data from the General Social Survey to see how well synthetic panels replicate real responses to a categorical question. Our insight? Although some models did better than others, the best responses were about as good as the results from a machine learning algorithm (random forest) trained with data from 3000 respondents.\n\nNow we\u2019re ready for a more challenging test, generating open-ended text.\n\nIdentifying the Most Important Problems\n\nWe use data from the American National Election Studies (ANES)2020 Time Series Study, which includes several open-ended questions. The one we\u2019ll look at is \u201cWhat do you think are the most important problems facing this country?\u201d We chose this question because we expect the responses to be moderately predictable based on demographic information about the respondents.\n\nTo see whether the responses we get from synthetic consumers are consistent with real people, we randomly selected a test set of 100 respondents. For each respondent, we collected demographic information including age, gender, race, education, income, occupation, and religious affiliation, as well as responses to the following question about political alignment, \u201cWhere would you place yourself on this scale?\u201d  from extremely liberal--point 1--to extremely conservative--point 7.\n\nTo test synthetic consumers, we composed a prompt with three elements:\n\nInstructions for the LLM to simulate a respondent with the given demographic profile,\n\nThe text of the ANES question about the most important problems facing the country (quoted above), and\n\nInstructions to generate \u201ca short open-ended text response that is consistent in content and style with this information about you, and reflects the conditions and topics of discussion in 2020.\u201d\n\nAsking the LLMs to generate responses as if they were asked in 2020 adds an additional challenge to the task. We will see that some models are better at it than others.\n\nTo evaluate the responses, we used the LLMs themselves:\n\nFirst we collected batches of the real responses and asked LLMs to identify recurring issues. Then we compiled the results into a list of 18 issues.\n\nNext we iterated through the real responses and prompted an LLM to identify the issues mentioned in the responses. Based on spot checks, these classifications were consistent with our own interpretation. And they were able to correctly classify responses in Spanish as well as responses with non-standard spelling and grammar.\n\nFinally, we iterated through the generated responses and classified them in the same way.\n\nTo evaluate the quality of the synthetic responses, we compute the percentage of real and synthetic respondents who mentioned each issue. The following figure shows the results for one of the models with the best results.\n\nThe synthetic results have been scaled to have the same average as the actual results. Although the prompt instructed models to describe no more than three problems, the synthetic responses often included more. In contrast, real respondents mentioned an average of 1.7 problems. We think scaling the results is appropriate because replicating the number of problems real people mention is not an essential part of the task \u2013 more important is to replicate the distribution of responses.\n\nTo quantify the alignment of the two distributions, we compute these metrics:\n\nMean Absolute Error (MAE): measures the average difference in issue frequencies between synthetic and actual responses.\n\nKendall\u2019s Tau: measures how well the relative orderings of issue importance align between the two distributions.\n\nSpearman\u2019s Rank Correlation: captures the overall consistency in the rank orderings of the issues.\n\nThe table below shows the models with the lowest MAE, indicating the closest match to actual issue frequencies.\n\nThe following table highlights the models with the highest Kendall\u2019s tau values, indicating the strongest agreement in issue rankings.\n\nTop-performing models by both metrics include GPT-4 (OpenAI), Claude 3.7 Sonnet (Anthropic), and DeepSeek R1.\n\nHere are the detailed results from the model with the highest value of tau, Meta Llama 4 Maverick:\n\nEven in the cases with the best metrics, there are hits and misses. In this example, none of the synthetic responses mentioned poverty and homelessness or election integrity. And the model overestimates mentions of the economy and healthcare access.\n\nAlmost every model overestimated mentions of healthcare access, an issue that became more salient since the data were collected in 2020. This suggests that the models have limited ability to give responses that reflect \u201cthe conditions and topics of discussion in 2020\u201d, as they were instructed. However, in a typical use case for synthetic consumers, this kind of time travel is not necessary.\n\nConclusions\n\nThese results suggest that synthetic consumers based on large language models (LLMs) are capable of generating open-ended responses that meaningfully reflect the concerns of real people. While none of the models perfectly replicated the distribution of issues mentioned by respondents in the 2020 ANES survey, several came surprisingly close\u2014especially in reproducing therelative salienceof different problems.\n\nModels likeDeepSeek R1,Claude 3.7 Sonnet, andGPT-4performed best across both metrics: they not only estimated issue frequencies with low error but also preserved the ranking of concerns found in real responses. These findings support a broader generalization:the best-performing models are not necessarily the largest, but those that appear to reason more carefully and follow instructions more reliably. Models tuned for alignment and context sensitivity\u2014 reasoning models\u2014generally outperform others in this kind of task.\n\nAt the same time, most models were not able to satisfy thetemporal constraintin the prompt. Many overestimated the salience of issues that gained visibility after 2020, such as healthcare access and inflation. However, in many practical applications, time-specific realism may not be essential.\n\nThese results support cautious optimism about using LLM-based synthetic consumers in marketing and qualitative research. They can generate realistic open-ended responses quickly, making it easier to test survey questions, explore audience reactions, or get an early read on how different groups might respond to a campaign.",
    "word_count": 1055
  },
  {
    "title": "The AI MMM Agent, An AI-Powered Shortcut to Bayesian Marketing Mix Insights",
    "url": "https://www.pymc-labs.com/blog-posts/the-ai-mmm-agent",
    "date": null,
    "author": null,
    "description": "AI revolutionizes marketing analytics by dramatically accelerating the traditional marketing mix modeling (MMM) process. This AI agent automates PyMC-Marketing workflows, delivering MMM results in hours instead of months.",
    "content": "The AI MMM Agent, An AI-Powered Shortcut to Bayesian Marketing Mix Insights\n\nSeptember 23, 2025\n\nByLuca Fiaschi\n\nAbstract:\n\nWhat if you could transform raw spend data into boardroom strategy in just one day? For CMOs, waiting months for marketing mix modeling (MMM) results is no longer an option. AI is revolutionizing the marketing analytics industry by dramatically accelerating the traditional modeling and insight workflow. In this post, we showcase an AI agent that automates PyMC-Marketing workflows\u2014delivering MMM results in hours instead of months. Discover how it converts raw spend data into boardroom-ready strategy while balancing technical rigor with executive clarity.\n\nThe Traditional MMM Bottleneck: Why Teams Get Stuck\n\nBuilding a Bayesian marketing mix model today often feels like assembling a plane mid-flight. The process is fraught with obstacles that slow progress at every turn:\n\nData Prep Chaos:Teams frequently spend weeks merging siloed data\u2014from sales and ads to promotions\u2014while wrestling with discrepancies. Questions like, \u201cAre these TikTok spend numbers from Q2 accurate?\u201d or \u201cWhy does the CRM data conflict with GA4?\u201d are common. These gaps and inconsistencies require experienced analysts to identify issues and implement painstaking manual fixes.\n\nModel-Building Guesswork:Once the data is ready, the challenge shifts to model configuration. Data scientists grapple with decisions such as whether the adstock decay should be 30 or 45 days, or if saturation is best modeled using a Hill curve or an exponential function. This trial-and-error approach can involve testing over ten different configurations while battling issues like multicollinearity and the risk of overfitting.\n\nValidation Black Holes:Even with a model in place, validation can become a black hole. When the MCMC sampling won\u2019t converge, experts are left questioning whether the problem lies with the priors or the underlying spend data. Debugging often consumes more time than building the model itself.\n\nStale Insights Syndrome:By the time a model is fully built and validated, the market may have already moved on. In fact, industry reports have highlighted that the lengthy process involved in building and updating MMM can severely limit their impact and adoption.\n\nEach of these phases requires time and expertise. In fact, you typically need a team of data scientists, analysts, and engineers to cover all these bases\u200b\u2013 talent that is expensive and in short supply\u200b.\n\nIn addition, this series of bottlenecks not only wastes valuable time and resources but also risks delivering outdated insights. This is where our AI MMM Agent steps in to transform the process.\n\nAI Agent: From Data to Dollars\n\nThe traditional MMM workflow demands specialized expertise and considerable time\u2014but what if AI could do it all? Enter the AI agent, your MMM copilot that compresses weeks of work into mere hours. Leveraging the open-sourcePyMC-Marketinglibrary from PyMC Labs, this agent automates the end-to-end process of building and running a Bayesian marketing mix model.\n\nKey Capabilities of the AI Copilot\n\nGuided data exploration:The agent assists in accessing and cleaning your data, suggesting tailored visualizations and diagnostics. Combining the expertise of an experienced data scientist with deep knowledge of your specific data context, it uncovers key descriptive insights that might otherwise be missed.\n\nGuided data exploration:The agent assists in accessing and cleaning your data, suggesting tailored visualizations and diagnostics. Combining the expertise of an experienced data scientist with deep knowledge of your specific data context, it uncovers key descriptive insights that might otherwise be missed.\n\nSmart model configuration:Based on your dataset and business context, the AI selects an optimal model structure. For example, if a long time series with underlying trends is detected, the agent enables a time-varying baseline. By harnessing PyMC-Marketing\u2019s high-level API, it instantiates an MMM with the right components (carryover, saturation, seasonality) without the need for extensive manual coding.\n\nSmart model configuration:Based on your dataset and business context, the AI selects an optimal model structure. For example, if a long time series with underlying trends is detected, the agent enables a time-varying baseline. By harnessing PyMC-Marketing\u2019s high-level API, it instantiates an MMM with the right components (carryover, saturation, seasonality) without the need for extensive manual coding.\n\nFast Bayesian inference:The agent fits the model to your data using PyMC\u2019s efficient sampling methods. With optimizations like custom adstock calculations running in linear time and GPU sampling, the entire process is significantly faster than other implementations.\n\nFast Bayesian inference:The agent fits the model to your data using PyMC\u2019s efficient sampling methods. With optimizations like custom adstock calculations running in linear time and GPU sampling, the entire process is significantly faster than other implementations.\n\nAutomated insight delivery:The final output isn\u2019t just a static presentation\u2014it\u2019s an interactive expert that translates complex posterior estimates into clear, actionable takeaways and visuals. For instance, it might output:\u201cFacebook ads drove an estimated 20% of sales with a 4.5\u00d7 ROI last quarter. Consider shifting the budget from print to Facebook, which could boost overall ROI by X%.\u201d\nThis immediacy and clarity let you bypass manual number-crunching and jump straight to strategy, while enabling live follow-up questions for further exploration.By streamlining data preparation, model building, and interpretation, the AI agentdramatically shortens the time-to-insight. What once took months can now happen in real time\u2014saving costs, reducing labor, and enabling frequent, up-to-date MMM analyses and follow up questions answered in minutes.\n\nAutomated insight delivery:The final output isn\u2019t just a static presentation\u2014it\u2019s an interactive expert that translates complex posterior estimates into clear, actionable takeaways and visuals. For instance, it might output:\n\n\u201cFacebook ads drove an estimated 20% of sales with a 4.5\u00d7 ROI last quarter. Consider shifting the budget from print to Facebook, which could boost overall ROI by X%.\u201d\nThis immediacy and clarity let you bypass manual number-crunching and jump straight to strategy, while enabling live follow-up questions for further exploration.By streamlining data preparation, model building, and interpretation, the AI agentdramatically shortens the time-to-insight. What once took months can now happen in real time\u2014saving costs, reducing labor, and enabling frequent, up-to-date MMM analyses and follow up questions answered in minutes.\n\nUnder the Hood: AI\u2019s Interaction with Causal Models\n\nThe AI agent isn\u2019t a generic AutoML tool; it\u2019s designed specifically for marketing mix modeling, with an emphasis on causal insights. Beyond speed, our AI agent also ensures the integrity of your insights by embedding causal reasoning into the model:\n\nCausal structure awareness:To answer \u201cwhat-if\u201d questions, an MMM must capture causal relationships (not just correlations). The AI agent ensures the model includes appropriate control variables and reflects a causal DAG of your marketing ecosystem\u200b. For example, it will account for things like economic trends or competitor actions if those data are provided, so that channel effects are isolated. This causal design underpins reliable scenario planning (e.g., \u201cIf we cut email spend by 20%, we expect a 5% drop in sales\u201d).\n\nExperiment calibration (lift tests):The agent can incorporate results from lift test experiments directly into the modeling process. PyMC-Marketing allows you to add lift test measurements to an MMM before fitting\u200b. By calibrating the model with known incremental lift from experiments, the AI grounds the MMM in real-world cause-and-effect\u200b. This approach helps correct for unobserved confounding and biases in the observational data\u200b. In short, your MMM doesn\u2019t exist in a vacuum \u2013 it aligns with any experimental evidence you have, leading to more trustworthy recommendations. For instance you could set prior in words such as \u201cI believe the incremental CAC of facebook is between200; is this data consistent with my belief?\u201d\n\nBy acting as a sparring partner to intelligently configure these aspects, the AI agent delivers a robust Bayesian MMM that acts as a true causal decision tool\u2014aligning statistical rigor with business reality to provide accurate, actionable insights.\n\nWhy Marketers Love It: From Data to Action\n\nWhat do marketers and executives stand to gain from this AI-accelerated approach? Here are the headline benefits:\n\nFor Data Scientists:\n\nDramatically Reduce Grunt Work:By automating tedious tasks such as data validation, model configuration, and diagnostics, the AI agent cuts up to 80% of manual effort. This frees you up to concentrate on developing strategic insights rather than getting bogged down in the technical details.\n\nReal-Time \u201cWhat If?\u201d Analysis:The agent lets you test scenarios on the fly. Imagine asking, \u201cWhat happens if we double Amazon Ads during Prime Day?\u201d and receiving instant, actionable feedback. This agility means you can adapt strategies as market conditions change\u2014live and in real time.\n\nFor Executives:\n\nRapid Model Updates for Agile Budgeting:No more waiting months for model insights. With our AI-powered approach, you can update budgets weekly instead of quarterly, ensuring that your decisions are always based on the most current data.\n\nClarity, Not Jargon:Forget the technical details. The AI agent translates complex outputs into straightforward, boardroom-ready recommendations like, \u201cStop overspending on saturated channels\u2014here\u2019s your optimal mix.\u201d This clarity empowers you to make confident, data-driven decisions without getting lost in the details.\n\nIn essence, AI-powered MMM transforms what used to be a complex analytical exercise into an ongoing strategic asset. Marketers gain granular insights to fine-tune campaigns, and executives receive a high-level expert assistant \u2013nota dashboard! \u2013 that highlights what\u2019s driving ROI. The result is better-performing marketing and a unified strategy for future investments.\n\nScaling Up: The Bayesian AI Lab for Cross-Sectional Insights\n\nAgentic workflows are revolutionizing industries\u2014from marketing analytics to data science. At PyMC Labs, we\u2019re at the forefront of this innovation, partnering with clients to develop tailored solutions that automate complex analytics workflows. Although the MMM agent can be used as an off-the-shelf solution, it can also be integrated with other agents to automate end-to-end business processes. For instance, the MMM agent can work alongside an inventory management agent that oversees order processing or a sales promotion agent that optimizes the mix of marketing, pricing, and inventory to maximize ROI\u2014all while managing campaign execution.\n\nReady to Unlock More ROI from Your Marketing Mix?\n\nAn AI-powered MMM shortcut might be the game-changer your team needs. Contact us today to access this cutting-edge solution and start making data-driven marketing decisions faster and smarter.",
    "word_count": 1638
  },
  {
    "title": "The Quickest Migration Guide Ever from PyMC3 to PyMC v4.0",
    "url": "https://www.pymc-labs.com/blog-posts/the-quickest-migration-guide-ever-from-pymc3-to-pymc-v40",
    "date": null,
    "author": null,
    "description": "Discover how few changes are needed to upgrade to PyMC v4.0!",
    "content": "The Quickest Migration Guide Ever from PyMC3 to PyMC v4.0\n\nSeptember 23, 2025\n\nByEric J. Ma\n\nPyMC v4.0is out in beta!\nPyMC Labs is thrilled to have worked with the PyMC development team\nto get this release out the door.\nUsually, a new version implies code changes.\nIn this short post, we\u2019d like to highlight\nhow to migrate your code from PyMC3 to PyMC v4.0.\n\nThe tl;dr?\nWhile there\u2019s a lot of really cool improvements we\u2019ve made underneath the hood\n(stay tuned for a full post on this),\nthe number of things you\u2019ll need to change areminimal!\n\nInstallation\n\nPyMC v4.0 is now available viapipandconda.\n\nTo install bypip:\n\n(The--preflag is necessary to install the pre-release\nbefore we release the a production-ready version.)\n\nTo install byconda:\n\nImports\n\nThe biggest change you\u2019ll see is probably this one.\nYou probably imported PyMC3 as follows:\n\nWith PyMC v4.0, you\u2019ll do the following import instead:\n\nSampling\n\nIn PyMC3, we used to return aMultiTraceobject.\n\nIn PyMC v4.0, we instead return an ArviZInferenceDataobject instead:\n\nUnderneath the hood, theInferenceDataobject is a richer data structure\nthan the MultiTrace object,\nand is essentially a wrapper around xarray.\nXarray provides the ability to do much saner indexing and labelling of coordinates.\nYou can think of xarray as \u201cpandas for n-dimensional arrays\u201d.\nYou can read more about theInferenceDataobject\nonthe official ArviZ docs.\n\nIn addition to that theInferenceDataobject supports storing\nprior predictive and posterior predictive samples\nthat you can use to sanity-check your models.\n\nIf you do this often, stay tuned for API updates to PyMC v4.0:we are discussingmaking prior and posterior predictive samples a standard part ofpm.sample()itself!\n\nArviZ\n\nIn the past, you may have used PyMC3\u2019s built-in visualization capabilities:\n\nSince PyMC3, we\u2019ve started delegating these visualization capabilities to ArviZ,\nand as of PyMC v4.0, it\u2019s considered idiomatic to just use ArviZ instead.\n\nArviZ has a rich library of\nGood Bayesian Workflow-supportingvisual diagnostics,\nand we\u2019d encourage you to check it out!\n\nWhat about all of those custom things?\n\nPerhaps you\u2019ve made something custom using the old Theano library,\nand are unsure about what to do there?\n\nWell, Theano has been replaced by Aesara,\nand it\u2019s basically a one-to-one replacement (with minor exceptions).\n\nFor those who are curious,in one Greek tradition,\nAesara is the daughter of Theano and Pythagoras.\n\nAesara and Theano are both symbolic tensor libraries.\nSinceMILA\u2019s discontinued efforts developing Theano,\nthe PyMC dev team took over Theano and,\nwith the upgrades made thus far,\nwe\u2019ve repackaged it into Aesara.\nYou can check out theAesara docsandGitHub organizationto learn more about PyMC\u2019s backing library.\nWhile previously Theano was backed by C++,\nAesara adds Numba and JAX backends while striving for 100% NumPy API compatibility.\nIt\u2019s an exciting time to be a Pythonista!\n\nAnd if there\u2019s something that\u2019s really custom that you need,\nsuch as a custom JAX-backed operation or high performance custom code in Aesara,\nthenPyMC Labsis here to help!\n\nSummary\n\nIn summary, for the vast majority of users of PyMC3,\nmigrating from PyMC3 to PyMC v4.0 is a matter of switching a few lines of code.\nA few find-and-replace operations throughout your repository,\nand you should be fine.\nFor the more advanced users, you can use Aesara like Theano.\nAnd for those really technically challenging yet high business value problems,\nPyMC Labs is here for you \u2014 just get in touch with us \ud83d\ude42!",
    "word_count": 550
  },
  {
    "title": "Write Me a PyMC Model",
    "url": "https://www.pymc-labs.com/blog-posts/write-me-a-pymc-model",
    "date": null,
    "author": null,
    "description": "ModelCraft is an AI agent that writes, checks, and refines PyMC models \u2014 turning natural language prompts into validated Bayesian code. Built during a PyMC Labs hackathon, it combines LLMs, LangGraph, and a secure compiler to eliminate hallucinated functions and broken models, making Bayesian modeling faster and easier for all.",
    "content": "Write Me a PyMC Model\n\nSeptember 23, 2025\n\nByBernard (Ben) Mares,Allen Downey,Alexander Fengler\n\nHacking an agent to generate hallucination-free Bayesian models\n\nWriting a PyMC model is not easy. It requires familiarity with both statistical distributions and Python libraries. And if at first you don\u2019t succeed, debugging can be a challenge.\n\nLarge language models like GPT-4 are surprisingly capable at generating probabilistic models. You can give them a few sentences describing a Bayesian problem, and they will often respond with runnable PyMC code. But, as anyone who\u2019s tried this knows, the quality of the results varies \u2013 and demonstrates a variety of failure modes.\n\nSometimes the code is out of date \u2014 usingpymc3imports or old syntax. Sometimes it violates best practices, and occasionally the model won\u2019t even compile, thanks to hallucinated functions or subtle shape mismatches.\n\nWe wanted to fix that.\n\nSo, during our internalProbabilistic AI hackathon, we builtModelCraft\u2014 an LLM-powered modelling agent that doesn't just generate PyMC code, but alsochecks its own work. Before it returns anything to the user, it runs the model through a remote compiler, catches errors, and rewrites the model if necessary. The goal: make it easier for beginners and experts alike to go from modelling ideas to validated PyMC code \u2014 all through natural language.\n\nHow it works\n\nModelCraft acts as a simple conversational agent that helps users write and validate PyMC models through natural language. You describe the model you want \u2014 the context, priors, data, whatever \u2014 and the agent responds with a complete PyMC model that\u2019s been checked for correctness, at least in the sense that it compiles.\n\nBehind the scenes ModelCraft uses an LLM and LangGraph to generate candidate models and evaluate them using a secure code sandbox. This isn\u2019t just code generation \u2014 the agent actually compiles each model before returning it, so you\u2019re never stuck debugging a hallucinatedpm.MagicUnicorn.\n\nWe also built a custom tool calledcompile_pymc_modelthat runs inside a remote E2B sandbox. This tool takes PyMC code, attempts to compile it (without sampling), and returns either a success message or a detailed error traceback. That output is routed back to the agent, which uses it to decide whether to revise the model or move on.\n\nThe agent is exposed through a FastAPI server with an OpenAPI interface, and we connected it to a frontend built with Onyx. The result is a fluid workflow where users can type modeling problems in plain language, and get structured, idiomatic PyMC models in return \u2014 already validated, and ready for sampling or refinement.\n\nHow it\u2019s going\n\nIn our demo,ModelCraftsuccessfully tackled several medium-complexity modeling tasks \u2014 including problems drawn fromThink Bayes. The agent was able to generate valid, modern PyMC models with good structure and style, and thanks to its built-in validation step, it avoided many of the common pitfalls we see when using LLMs for code generation.\n\nIn some cases, it took the agent multiple iterations to arrive at a model that compiled cleanly. This added some latency, but it also highlighted the value of the retry-and-validate loop \u2014 instead of handing off a broken model to the user, the agent caught its own mistakes and improved on them.\n\nWe found, as others have noted in the literature before, thatsubtle changes in the system promptcan make abig differencein the quality of the model produced . For example, the \u201clions and tigers and bears\u201d problem fromThink Bayescan be elegantly modeled with a Dirichlet distribution. However, when the system prompt included an instruction to use Python type annotations, most of the models produced involved a convoluted solution involving categorical distributions because the extra steps involved allowed for the use of type annotations. Model quality improved substantially after we removed this instruction.\n\nThis line generates an error because the keyword arguments ofHyperGeometricare not correct.\n\nWith this information, ModelCraft generates a second iteration that compiles and samples:\n\nThis is not the only solution to the problem. In particular, we might want to iterate on the form of the prior distribution. But it helps to start with a correct model!\n\nFuture Direction\n\nModel Craft was developed during an internal hackathon to help our team deliver client solutions more efficiently when coding complex Bayesian models. For a one-day project, we think it\u2019s a compelling proof of concept: an AI modeling assistant that\u2019s not just smart, but careful. The experience underscored the immense power we can unlock with today\u2019s LLM orchestration tools\u2014even with relatively minimal effort.\n\nGiven more time, we plan to extend Model Craft to support full model diagnostics, posterior visualization, and user-driven refinement loops. Another promising direction is wrapping this agent in an MCP server to communicate with Claude, Cursor, and other MCP clients. This will allow PyMC developers to use AI-powered assistant IDEs more effectively, all within their preferred coding environment.\n\nWe\u2019ll soon be releasing a full MCP server for PyMC\u2014so stay tuned!\n\nWorking with PyMC\n\nIf you are interested in seeing what we at PyMC Labs can do for you, then please emailinfo@pymc-labs.com. We work with companies at a variety of scales and with varying levels of existing modeling capacity. We also run corporate workshop training events and can provide sessions ranging from introduction to Bayes to more advanced topics.",
    "word_count": 863
  }
]